<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>NettyIO模型</title>
      <link href="/2024/09/28/NettyIO%E6%A8%A1%E5%9E%8B/"/>
      <url>/2024/09/28/NettyIO%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>从今天开始我们来聊聊Netty的那些事儿，我们都知道Netty是一个高性能异步事件驱动的网络框架。</p><p>它的设计异常优雅简洁，扩展性高，稳定性强。拥有非常详细完整的用户文档。</p><p>同时内置了很多非常有用的模块基本上做到了开箱即用，用户只需要编写短短几行代码，就可以快速构建出一个具有<code>高吞吐</code>，<code>低延时</code>，<code>更少的资源消耗</code>，<code>高性能（非必要的内存拷贝最小化）</code>等特征的高并发网络应用程序。</p><p>本文我们来探讨下支持Netty具有<code>高吞吐</code>，<code>低延时</code>特征的基石—-netty的<code>网络IO模型</code>。</p><p>由Netty的<code>网络IO模型</code>开始，我们来正式揭开本系列Netty源码解析的序幕：</p><h2 id="1、网络包接收流程"><a href="#1、网络包接收流程" class="headerlink" title="1、网络包接收流程"></a>1、网络包接收流程</h2><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640.png" alt="图片">                 </p><ul><li>当<code>网络数据帧</code>通过网络传输到达网卡时，网卡会将网络数据帧通过<code>DMA的方式</code>放到<code>环形缓冲区RingBuffer</code>中。</li></ul><blockquote><p><code>RingBuffer</code>是网卡在启动的时候<code>分配和初始化</code>的<code>环形缓冲队列</code>。当<code>RingBuffer满</code>的时候，新来的数据包就会被<code>丢弃</code>。我们可以通过<code>ifconfig</code>命令查看网卡收发数据包的情况。其中<code>overruns</code>数据项表示当<code>RingBuffer满</code>时，被<code>丢弃的数据包</code>。如果发现出现丢包情况，可以通过<code>ethtool命令</code>来增大RingBuffer长度。</p></blockquote><ul><li>当<code>DMA操作完成</code>时，网卡会向CPU发起一个<code>硬中断</code>，告诉<code>CPU</code>有网络数据到达。CPU调用网卡驱动注册的<code>硬中断响应程序</code>。网卡硬中断响应程序会为网络数据帧创建内核数据结构<code>sk_buffer</code>，并将网络数据帧<code>拷贝</code>到<code>sk_buffer</code>中。然后发起<code>软中断请求</code>，通知<code>内核</code>有新的网络数据帧到达。</li></ul><blockquote><p><code>sk_buff</code>缓冲区，是一个维护网络帧结构的<code>双向链表</code>，链表中的每一个元素都是一个<code>网络帧</code>。虽然 TCP&#x2F;IP 协议栈分了好几层，但上下不同层之间的传递，实际上只需要操作这个数据结构中的指针，而<code>无需进行数据复制</code>。</p></blockquote><ul><li>内核线程<code>ksoftirqd</code>发现有软中断请求到来，随后调用网卡驱动注册的<code>poll函数</code>，<code>poll函数</code>将<code>sk_buffer</code>中的<code>网络数据包</code>送到内核协议栈中注册的<code>ip_rcv函数</code>中。</li></ul><blockquote><p><code>每个CPU</code>会绑定<code>一个ksoftirqd</code>内核线程<code>专门</code>用来处理<code>软中断响应</code>。2个 CPU 时，就会有 <code>ksoftirqd/0</code> 和 <code>ksoftirqd/1</code>这两个内核线程。</p></blockquote><blockquote><p><strong>这里有个事情需要注意下：</strong> 网卡接收到数据后，当<code>DMA拷贝完成</code>时，向CPU发出<code>硬中断</code>，这时<code>哪个CPU</code>上响应了这个<code>硬中断</code>，那么在网卡<code>硬中断响应程序</code>中发出的<code>软中断请求</code>也会在<code>这个CPU绑定的ksoftirqd线程</code>中响应。所以如果发现Linux软中断，CPU消耗都<code>集中在一个核上</code>的话，那么就需要调整硬中断的<code>CPU亲和性</code>，来将硬中断<code>打散</code>到<code>不通的CPU核</code>上去。</p></blockquote><ul><li>在<code>ip_rcv函数</code>中也就是上图中的<code>网络层</code>，<code>取出</code>数据包的<code>IP头</code>，判断该数据包下一跳的走向，如果数据包是发送给本机的，则取出传输层的协议类型（<code>TCP</code>或者<code>UDP</code>)，并<code>去掉</code>数据包的<code>IP头</code>，将数据包交给上图中得<code>传输层</code>处理。</li></ul><blockquote><p>传输层的处理函数：<code>TCP协议</code>对应内核协议栈中注册的<code>tcp_rcv函数</code>，<code>UDP协议</code>对应内核协议栈中注册的<code>udp_rcv函数</code>。</p></blockquote><ul><li>当我们采用的是<code>TCP协议</code>时，数据包到达传输层时，会在内核协议栈中的<code>tcp_rcv函数</code>处理，在tcp_rcv函数中<code>去掉</code>TCP头，根据<code>四元组（源IP，源端口，目的IP，目的端口）</code>查找<code>对应的Socket</code>，如果找到对应的Socket则将网络数据包中的传输数据拷贝到<code>Socket</code>中的<code>接收缓冲区</code>中。如果没有找到，则发送一个<code>目标不可达</code>的<code>icmp</code>包。</li><li>内核在接收网络数据包时所做的工作我们就介绍完了，现在我们把视角放到应用层，当我们程序通过系统调用<code>read</code>读取<code>Socket接收缓冲区</code>中的数据时，如果接收缓冲区中<code>没有数据</code>，那么应用程序就会在系统调用上<code>阻塞</code>，直到Socket接收缓冲区<code>有数据</code>，然后<code>CPU</code>将<code>内核空间</code>（Socket接收缓冲区）的数据<code>拷贝</code>到<code>用户空间</code>，最后系统调用<code>read返回</code>，应用程序<code>读取</code>数据。</li></ul><h2 id="2、性能开销"><a href="#2、性能开销" class="headerlink" title="2、性能开销"></a>2、性能开销</h2><p>从内核处理网络数据包接收的整个过程来看，内核帮我们做了非常之多的工作，最终我们的应用程序才能读取到网络数据。</p><p>随着而来的也带来了很多的性能开销，结合前面介绍的网络数据包接收过程我们来看下网络数据包接收的过程中都有哪些性能开销：</p><ul><li>应用程序通过<code>系统调用</code>从<code>用户态</code>转为<code>内核态</code>的开销以及系统调用<code>返回</code>时从<code>内核态</code>转为<code>用户态</code>的开销。</li><li>网络数据从<code>内核空间</code>通过<code>CPU拷贝</code>到<code>用户空间</code>的开销。</li><li>内核线程<code>ksoftirqd</code>响应<code>软中断</code>的开销。</li><li><code>CPU</code>响应<code>硬中断</code>的开销。</li><li><code>DMA拷贝</code>网络数据包到<code>内存</code>中的开销。</li></ul><h2 id="3、网络包发送流程"><a href="#3、网络包发送流程" class="headerlink" title="3、网络包发送流程"></a>3、网络包发送流程</h2><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727503906683-1.png" alt="图片">网络包发送过程.png</p><ul><li>当我们在应用程序中调用<code>send</code>系统调用发送数据时，由于是系统调用所以线程会发生一次用户态到内核态的转换，在内核中首先根据<code>fd</code>将真正的Socket找出，这个Socket对象中记录着各种协议栈的函数地址，然后构造<code>struct msghdr</code>对象，将用户需要发送的数据全部封装在这个<code>struct msghdr</code>结构体中。</li><li>调用内核协议栈函数<code>inet_sendmsg</code>，发送流程进入内核协议栈处理。在进入到内核协议栈之后，内核会找到Socket上的具体协议的发送函数。</li></ul><blockquote><p>比如：我们使用的是<code>TCP协议</code>，对应的<code>TCP协议</code>发送函数是<code>tcp_sendmsg</code>，如果是<code>UDP协议</code>的话，对应的发送函数为<code>udp_sendmsg</code>。</p></blockquote><ul><li>在<code>TCP协议</code>的发送函数<code>tcp_sendmsg</code>中，创建内核数据结构<code>sk_buffer</code>,将<code>struct msghdr</code>结构体中的发送数据<code>拷贝</code>到<code>sk_buffer</code>中。调用<code>tcp_write_queue_tail</code>函数获取<code>Socket</code>发送队列中的队尾元素，将新创建的<code>sk_buffer</code>添加到<code>Socket</code>发送队列的尾部。</li></ul><blockquote><p><code>Socket</code>的发送队列是由<code>sk_buffer</code>组成的一个<code>双向链表</code>。</p></blockquote><blockquote><p>发送流程走到这里，用户要发送的数据总算是从<code>用户空间</code>拷贝到了<code>内核</code>中，这时虽然发送数据已经<code>拷贝</code>到了内核<code>Socket</code>中的<code>发送队列</code>中，但并不代表内核会开始发送，因为<code>TCP协议</code>的<code>流量控制</code>和<code>拥塞控制</code>，用户要发送的数据包<code>并不一定</code>会立马被发送出去，需要符合<code>TCP协议</code>的发送条件。如果<code>没有达到发送条件</code>，那么本次<code>send</code>系统调用就会直接返回。</p></blockquote><ul><li>如果符合发送条件，则开始调用<code>tcp_write_xmit</code>内核函数。在这个函数中，会循环获取<code>Socket</code>发送队列中待发送的<code>sk_buffer</code>，然后进行<code>拥塞控制</code>以及<code>滑动窗口的管理</code>。</li><li>将从<code>Socket</code>发送队列中获取到的<code>sk_buffer</code>重新<code>拷贝一份</code>，设置<code>sk_buffer副本</code>中的<code>TCP HEADER</code>。</li></ul><blockquote><p><code>sk_buffer</code> 内部其实包含了网络协议中所有的 <code>header</code>。在设置 <code>TCP HEADER</code>的时候，只是把指针指向 <code>sk_buffer</code>的合适位置。后面再设置 <code>IP HEADER</code>的时候，在把指针移动一下就行，避免频繁的内存申请和拷贝，效率很高。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727503906683-2.png" alt="图片"></p><blockquote><p><strong>为什么不直接使用<code>Socket</code>发送队列中的<code>sk_buffer</code>而是需要拷贝一份呢？</strong>因为<code>TCP协议</code>是支持<code>丢包重传</code>的，在没有收到对端的<code>ACK</code>之前，这个<code>sk_buffer</code>是不能删除的。内核每次调用网卡发送数据的时候，实际上传递的是<code>sk_buffer</code>的<code>拷贝副本</code>，当网卡把数据发送出去后，<code>sk_buffer</code>拷贝副本会被释放。当收到对端的<code>ACK</code>之后，<code>Socket</code>发送队列中的<code>sk_buffer</code>才会被真正删除。</p></blockquote><ul><li><p>当设置完<code>TCP头</code>后，内核协议栈<code>传输层</code>的事情就做完了，下面通过调用<code>ip_queue_xmit</code>内核函数，正式来到内核协议栈<code>网络层</code>的处理。</p><blockquote><p>通过<code>route</code>命令可以查看本机路由配置。</p></blockquote><blockquote><p>如果你使用 <code>iptables</code>配置了一些规则，那么这里将检测<code>是否命中</code>规则。如果你设置了非常<code>复杂的 netfilter 规则</code>，在这个函数里将会导致你的线程 <code>CPU 开销</code>会<code>极大增加</code>。</p></blockquote><ul><li>将<code>sk_buffer</code>中的指针移动到<code>IP头</code>位置上，设置<code>IP头</code>。</li><li>执行<code>netfilters</code>过滤。过滤通过之后，如果数据大于 <code>MTU</code>的话，则执行分片。</li><li>检查<code>Socket</code>中是否有缓存路由表，如果没有的话，则查找路由项，并缓存到<code>Socket</code>中。接着在把路由表设置到<code>sk_buffer</code>中。</li></ul></li><li><p>内核协议栈<code>网络层</code>的事情处理完后，现在发送流程进入了到了<code>邻居子系统</code>，<code>邻居子系统</code>位于内核协议栈中的<code>网络层</code>和<code>网络接口层</code>之间，用于发送<code>ARP请求</code>获取<code>MAC地址</code>，然后将<code>sk_buffer</code>中的指针移动到<code>MAC头</code>位置，填充<code>MAC头</code>。</p></li><li><p>经过<code>邻居子系统</code>的处理，现在<code>sk_buffer</code>中已经封装了一个完整的<code>数据帧</code>，随后内核将<code>sk_buffer</code>交给<code>网络设备子系统</code>进行处理。<code>网络设备子系统</code>主要做以下几项事情：</p><ul><li>选择发送队列（<code>RingBuffer</code>）。因为网卡拥有多个发送队列，所以在发送前需要选择一个发送队列。</li><li>将<code>sk_buffer</code>添加到发送队列中。</li><li>循环从发送队列（<code>RingBuffer</code>）中取出<code>sk_buffer</code>，调用内核函数<code>sch_direct_xmit</code>发送数据，其中会调用<code>网卡驱动程序</code>来发送数据。</li></ul></li></ul><blockquote><p>以上过程全部是用户线程的内核态在执行，占用的CPU时间是系统态时间(<code>sy</code>)，当分配给用户线程的<code>CPU quota</code>用完的时候，会触发<code>NET_TX_SOFTIRQ</code>类型的软中断，内核线程<code>ksoftirqd</code>会响应这个软中断，并执行<code>NET_TX_SOFTIRQ</code>类型的软中断注册的回调函数<code>net_tx_action</code>，在回调函数中会执行到驱动程序函数 <code>dev_hard_start_xmit</code>来发送数据。</p></blockquote><blockquote><p><strong>注意：当触发<code>NET_TX_SOFTIRQ</code>软中断来发送数据时，后边消耗的 CPU 就都显示在 <code>si</code>这里了，不会消耗用户进程的系统态时间（<code>sy</code>）了。</strong></p></blockquote><blockquote><p>从这里可以看到网络包的发送过程和接受过程是不同的，在介绍网络包的接受过程时，我们提到是通过触发<code>NET_RX_SOFTIRQ</code>类型的软中断在内核线程<code>ksoftirqd</code>中执行<code>内核网络协议栈</code>接受数据。而在网络数据包的发送过程中是<code>用户线程的内核态</code>在执行<code>内核网络协议栈</code>，只有当线程的<code>CPU quota</code>用尽时，才触发<code>NET_TX_SOFTIRQ</code>软中断来发送数据。</p></blockquote><blockquote><p>在整个网络包的发送和接受过程中，<code>NET_TX_SOFTIRQ</code>类型的软中断只会在发送网络包时并且当用户线程的<code>CPU quota</code>用尽时，才会触发。剩下的接受过程中触发的软中断类型以及发送完数据触发的软中断类型均为<code>NET_RX_SOFTIRQ</code>。所以这就是你在服务器上查看 <code>/proc/softirqs</code>，一般 <code>NET_RX</code>都要比 <code>NET_TX</code>大很多的的原因。</p></blockquote><ul><li>现在发送流程终于到了网卡真实发送数据的阶段，前边我们讲到无论是用户线程的内核态还是触发<code>NET_TX_SOFTIRQ</code>类型的软中断在发送数据的时候最终会调用到网卡的驱动程序函数<code>dev_hard_start_xmit</code>来发送数据。在网卡驱动程序函数<code>dev_hard_start_xmit</code>中会将<code>sk_buffer</code>映射到网卡可访问的<code>内存 DMA 区域</code>，最终网卡驱动程序通过<code>DMA</code>的方式将<code>数据帧</code>通过物理网卡发送出去。</li><li>当数据发送完毕后，还有最后一项重要的工作，就是清理工作。数据发送完毕后，网卡设备会向<code>CPU</code>发送一个硬中断，<code>CPU</code>调用网卡驱动程序注册的<code>硬中断响应程序</code>，在硬中断响应中触发<code>NET_RX_SOFTIRQ</code>类型的软中断，在软中断的回调函数<code>igb_poll</code>中清理释放 <code>sk_buffer</code>，清理<code>网卡</code>发送队列（<code>RingBuffer</code>），解除 DMA 映射。</li></ul><blockquote><p>无论<code>硬中断</code>是因为<code>有数据要接收</code>，还是说<code>发送完成通知</code>，从硬中断触发的软中断都是 <code>NET_RX_SOFTIRQ</code>。</p></blockquote><blockquote><p>这里释放清理的只是<code>sk_buffer</code>的副本，真正的<code>sk_buffer</code>现在还是存放在<code>Socket</code>的发送队列中。前面在<code>传输层</code>处理的时候我们提到过，因为传输层需要<code>保证可靠性</code>，所以 <code>sk_buffer</code>其实还没有删除。它得等收到对方的 ACK 之后才会真正删除。</p></blockquote><h2 id="4、性能开销"><a href="#4、性能开销" class="headerlink" title="4、性能开销"></a>4、性能开销</h2><p>前边我们提到了在网络包接收过程中涉及到的性能开销，现在介绍完了网络包的发送过程，我们来看下在数据包发送过程中的性能开销：</p><ul><li>和接收数据一样，应用程序在调用<code>系统调用send</code>的时候会从<code>用户态</code>转为<code>内核态</code>以及发送完数据后，<code>系统调用</code>返回时从<code>内核态</code>转为<code>用户态</code>的开销。</li><li>用户线程内核态<code>CPU quota</code>用尽时触发<code>NET_TX_SOFTIRQ</code>类型软中断，内核响应软中断的开销。</li><li>网卡发送完数据，向<code>CPU</code>发送硬中断，<code>CPU</code>响应硬中断的开销。以及在硬中断中发送<code>NET_RX_SOFTIRQ</code>软中断执行具体的内存清理动作。内核响应软中断的开销。</li><li>内存拷贝的开销。我们来回顾下在数据包发送的过程中都发生了哪些内存拷贝：<ul><li>在内核协议栈的传输层中，<code>TCP协议</code>对应的发送函数<code>tcp_sendmsg</code>会申请<code>sk_buffer</code>，将用户要发送的数据<code>拷贝</code>到<code>sk_buffer</code>中。</li><li>在发送流程从传输层到网络层的时候，会<code>拷贝</code>一个<code>sk_buffer副本</code>出来，将这个<code>sk_buffer副本</code>向下传递。原始<code>sk_buffer</code>保留在<code>Socket</code>发送队列中，等待网络对端<code>ACK</code>，对端<code>ACK</code>后删除<code>Socket</code>发送队列中的<code>sk_buffer</code>。对端没有发送<code>ACK</code>，则重新从<code>Socket</code>发送队列中发送，实现<code>TCP协议</code>的可靠传输。</li><li>在网络层，如果发现要发送的数据大于<code>MTU</code>，则会进行分片操作，申请额外的<code>sk_buffer</code>，并将原来的sk_buffer<code>拷贝</code>到多个小的sk_buffer中。</li></ul></li></ul><h2 id="5、再谈-阻塞，非阻塞-与-同步，异步"><a href="#5、再谈-阻塞，非阻塞-与-同步，异步" class="headerlink" title="5、再谈(阻塞，非阻塞)与(同步，异步)"></a>5、再谈(阻塞，非阻塞)与(同步，异步)</h2><p>在我们聊完网络数据的接收和发送过程后，我们来谈下IO中特别容易混淆的概念：<code>阻塞与同步</code>，<code>非阻塞与异步</code>。</p><p>网上各种博文还有各种书籍中有大量的关于这两个概念的解释，但是笔者觉得还是不够形象化，只是对概念的生硬解释，如果硬套概念的话，其实感觉<code>阻塞与同步</code>，<code>非阻塞与异步</code>还是没啥区别，时间长了，还是比较模糊容易混淆。</p><p>所以笔者在这里尝试换一种更加形象化，更加容易理解记忆的方式来清晰地解释下什么是<code>阻塞与非阻塞</code>，什么是<code>同步与异步</code>。</p><p>经过前边对网络数据包接收流程的介绍，在这里我们可以将整个流程总结为两个阶段：</p><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504029972-9.png" alt="图片">数据接收阶段.png</p><ul><li><strong>数据准备阶段：</strong> 在这个阶段，网络数据包到达网卡，通过<code>DMA</code>的方式将数据包拷贝到内存中，然后经过硬中断，软中断，接着通过内核线程<code>ksoftirqd</code>经过内核协议栈的处理，最终将数据发送到<code>内核Socket</code>的接收缓冲区中。</li><li><strong>数据拷贝阶段：</strong> 当数据到达<code>内核Socket</code>的接收缓冲区中时，此时数据存在于<code>内核空间</code>中，需要将数据<code>拷贝</code>到<code>用户空间</code>中，才能够被应用程序读取。</li></ul><h2 id="6、阻塞与非阻塞"><a href="#6、阻塞与非阻塞" class="headerlink" title="6、阻塞与非阻塞"></a>6、阻塞与非阻塞</h2><p>阻塞与非阻塞的区别主要发生在第一阶段：<code>数据准备阶段</code>。</p><p>当应用程序发起<code>系统调用read</code>时，线程从用户态转为内核态，读取内核<code>Socket</code>的接收缓冲区中的网络数据。</p><h3 id="阻塞"><a href="#阻塞" class="headerlink" title="阻塞"></a>阻塞</h3><p>如果这时内核<code>Socket</code>的接收缓冲区没有数据，那么线程就会一直<code>等待</code>，直到<code>Socket</code>接收缓冲区有数据为止。随后将数据从内核空间拷贝到用户空间，<code>系统调用read</code>返回。</p><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504049520-12.png" alt="图片">阻塞IO.png</p><p>从图中我们可以看出：<strong>阻塞</strong>的特点是在第一阶段和第二阶段<code>都会等待</code>。</p><h3 id="非阻塞"><a href="#非阻塞" class="headerlink" title="非阻塞"></a>非阻塞</h3><p><code>阻塞</code>和<code>非阻塞</code>主要的区分是在第一阶段：<code>数据准备阶段</code>。</p><ul><li>在第一阶段，当<code>Socket</code>的接收缓冲区中没有数据的时候，<code>阻塞模式下</code>应用线程会一直等待。<code>非阻塞模式下</code>应用线程不会等待，<code>系统调用</code>直接返回错误标志<code>EWOULDBLOCK</code>。</li><li>当<code>Socket</code>的接收缓冲区中有数据的时候，<code>阻塞</code>和<code>非阻塞</code>的表现是一样的，都会进入第二阶段<code>等待</code>数据从<code>内核空间</code>拷贝到<code>用户空间</code>，然后<code>系统调用返回</code>。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504081954-15.png" alt="图片">非阻塞IO.png</p><p>从上图中，我们可以看出：<strong>非阻塞</strong>的特点是第一阶段<code>不会等待</code>，但是在第二阶段还是会<code>等待</code>。</p><h2 id="7、同步与异步"><a href="#7、同步与异步" class="headerlink" title="7、同步与异步"></a>7、同步与异步</h2><p><code>同步</code>与<code>异步</code>主要的区别发生在第二阶段：<code>数据拷贝阶段</code>。</p><p>前边我们提到在<code>数据拷贝阶段</code>主要是将数据从<code>内核空间</code>拷贝到<code>用户空间</code>。然后应用程序才可以读取数据。</p><p>当内核<code>Socket</code>的接收缓冲区有数据到达时，进入第二阶段。</p><h3 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h3><p><code>同步模式</code>在数据准备好后，是由<code>用户线程</code>的<code>内核态</code>来执行<code>第二阶段</code>。所以应用程序会在第二阶段发生<code>阻塞</code>，直到数据从<code>内核空间</code>拷贝到<code>用户空间</code>，系统调用才会返回。</p><p>Linux下的 <code>epoll</code>和Mac 下的 <code>kqueue</code>都属于<code>同步 IO</code>。</p><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504117164-22.png" alt="图片">同步IO.png</p><h3 id="异步"><a href="#异步" class="headerlink" title="异步"></a>异步</h3><p><code>异步模式</code>下是由<code>内核</code>来执行第二阶段的数据拷贝操作，当<code>内核</code>执行完第二阶段，会通知用户线程IO操作已经完成，并将数据回调给用户线程。所以在<code>异步模式</code>下 <code>数据准备阶段</code>和<code>数据拷贝阶段</code>均是由<code>内核</code>来完成，不会对应用程序造成任何阻塞。</p><p>基于以上特征，我们可以看到<code>异步模式</code>需要内核的支持，比较依赖操作系统底层的支持。</p><p>在目前流行的操作系统中，只有Windows 中的 <code>IOCP</code>才真正属于异步 IO，实现的也非常成熟。但Windows很少用来作为服务器使用。</p><p>而常用来作为服务器使用的Linux，<code>异步IO机制</code>实现的不够成熟，与NIO相比性能提升的也不够明显。</p><p>但Linux kernel 在5.1版本由Facebook的大神Jens Axboe引入了新的异步IO库<code>io_uring</code> 改善了原来Linux native AIO的一些性能问题。性能相比<code>Epoll</code>以及之前原生的<code>AIO</code>提高了不少，值得关注。</p><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504117164-23.png" alt="图片">异步IO.png</p><h2 id="8、IO模型"><a href="#8、IO模型" class="headerlink" title="8、IO模型"></a>8、IO模型</h2><p>在进行网络IO操作时，用什么样的IO模型来读写数据将在很大程度上决定了网络框架的IO性能。所以IO模型的选择是构建一个高性能网络框架的基础。</p><p>在《UNIX 网络编程》一书中介绍了五种IO模型：<code>阻塞IO</code>,<code>非阻塞IO</code>,<code>IO多路复用</code>,<code>信号驱动IO</code>,<code>异步IO</code>，每一种IO模型的出现都是对前一种的升级优化。</p><p>下面我们就来分别介绍下这五种IO模型各自都解决了什么问题，适用于哪些场景，各自的优缺点是什么？</p><h3 id="阻塞IO（BIO）"><a href="#阻塞IO（BIO）" class="headerlink" title="阻塞IO（BIO）"></a>阻塞IO（BIO）</h3><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504117164-24.png" alt="图片">阻塞IO.png</p><p>经过前一小节对<code>阻塞</code>这个概念的介绍，相信大家可以很容易理解<code>阻塞IO</code>的概念和过程。</p><p>既然这小节我们谈的是<code>IO</code>，那么下边我们来看下在<code>阻塞IO</code>模型下，网络数据的读写过程。</p><h4 id="阻塞读"><a href="#阻塞读" class="headerlink" title="阻塞读"></a>阻塞读</h4><p>当用户线程发起<code>read</code>系统调用，用户线程从用户态切换到内核态，在内核中去查看<code>Socket</code>接收缓冲区是否有数据到来。</p><ul><li><code>Socket</code>接收缓冲区中<code>有数据</code>，则用户线程在内核态将内核空间中的数据拷贝到用户空间，系统IO调用返回。</li><li><code>Socket</code>接收缓冲区中<code>无数据</code>，则用户线程让出CPU，进入<code>阻塞状态</code>。当数据到达<code>Socket</code>接收缓冲区后，内核唤醒<code>阻塞状态</code>中的用户线程进入<code>就绪状态</code>，随后经过CPU的调度获取到<code>CPU quota</code>进入<code>运行状态</code>，将内核空间的数据拷贝到用户空间，随后系统调用返回。</li></ul><h4 id="阻塞写"><a href="#阻塞写" class="headerlink" title="阻塞写"></a>阻塞写</h4><p>当用户线程发起<code>send</code>系统调用时，用户线程从用户态切换到内核态，将发送数据从用户空间拷贝到内核空间中的<code>Socket</code>发送缓冲区中。</p><ul><li>当<code>Socket</code>发送缓冲区能够容纳下发送数据时，用户线程会将全部的发送数据写入<code>Socket</code>缓冲区，然后执行在《网络包发送流程》这小节介绍的后续流程，然后返回。</li><li>当<code>Socket</code>发送缓冲区空间不够，无法容纳下全部发送数据时，用户线程让出CPU,进入<code>阻塞状态</code>，直到<code>Socket</code>发送缓冲区能够容纳下全部发送数据时，内核唤醒用户线程，执行后续发送流程。</li></ul><p><code>阻塞IO</code>模型下的写操作做事风格比较硬刚，非得要把全部的发送数据写入发送缓冲区才肯善罢甘休。</p><h4 id="阻塞IO模型"><a href="#阻塞IO模型" class="headerlink" title="阻塞IO模型"></a>阻塞IO模型</h4><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504117164-25.png" alt="图片">阻塞IO模型.png</p><p>由于<code>阻塞IO</code>的读写特点，所以导致在<code>阻塞IO</code>模型下，每个请求都需要被一个独立的线程处理。一个线程在同一时刻只能与一个连接绑定。来一个请求，服务端就需要创建一个线程用来处理请求。</p><p>当客户端请求的并发量突然增大时，服务端在一瞬间就会创建出大量的线程，而创建线程是需要系统资源开销的，这样一来就会一瞬间占用大量的系统资源。</p><p>如果客户端创建好连接后，但是一直不发数据，通常大部分情况下，网络连接也<code>并不</code>总是有数据可读，那么在空闲的这段时间内，服务端线程就会一直处于<code>阻塞状态</code>，无法干其他的事情。CPU也<code>无法得到充分的发挥</code>，同时还会<code>导致大量线程切换的开销</code>。</p><h4 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h4><p>基于以上<code>阻塞IO模型</code>的特点，该模型只适用于<code>连接数少</code>，<code>并发度低</code>的业务场景。</p><p>比如公司内部的一些管理系统，通常请求数在100个左右，使用<code>阻塞IO模型</code>还是非常适合的。而且性能还不输NIO。</p><p>该模型在C10K之前，是普遍被采用的一种IO模型。</p><h3 id="非阻塞IO（NIO）"><a href="#非阻塞IO（NIO）" class="headerlink" title="非阻塞IO（NIO）"></a>非阻塞IO（NIO）</h3><p><code>阻塞IO模型</code>最大的问题就是一个线程只能处理一个连接，如果这个连接上没有数据的话，那么这个线程就只能阻塞在系统IO调用上，不能干其他的事情。这对系统资源来说，是一种极大的浪费。同时大量的线程上下文切换，也是一个巨大的系统开销。</p><p>所以为了解决这个问题，<strong>我们就需要用尽可能少的线程去处理更多的连接。</strong>，<code>网络IO模型的演变</code>也是根据这个需求来一步一步演进的。</p><p>基于这个需求，第一种解决方案<code>非阻塞IO</code>就出现了。我们在上一小节中介绍了<code>非阻塞</code>的概念，现在我们来看下网络读写操作在<code>非阻塞IO</code>下的特点：</p><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504204257-34.png" alt="图片">非阻塞IO.png</p><h4 id="非阻塞读"><a href="#非阻塞读" class="headerlink" title="非阻塞读"></a>非阻塞读</h4><p>当用户线程发起非阻塞<code>read</code>系统调用时，用户线程从<code>用户态</code>转为<code>内核态</code>，在内核中去查看<code>Socket</code>接收缓冲区是否有数据到来。</p><ul><li><code>Socket</code>接收缓冲区中<code>无数据</code>，系统调用立马返回，并带有一个 <code>EWOULDBLOCK</code> 或 <code>EAGAIN</code>错误，这个阶段用户线程<code>不会阻塞</code>，也<code>不会让出CPU</code>，而是会继续<code>轮训</code>直到<code>Socket</code>接收缓冲区中有数据为止。</li><li><code>Socket</code>接收缓冲区中<code>有数据</code>，用户线程在<code>内核态</code>会将<code>内核空间</code>中的数据拷贝到<code>用户空间</code>，<strong>注意</strong>这个数据拷贝阶段，应用程序是<code>阻塞的</code>，当数据拷贝完成，系统调用返回。</li></ul><h4 id="非阻塞写"><a href="#非阻塞写" class="headerlink" title="非阻塞写"></a>非阻塞写</h4><p>前边我们在介绍<code>阻塞写</code>的时候提到<code>阻塞写</code>的风格特别的硬朗，头比较铁非要把全部发送数据一次性都写到<code>Socket</code>的发送缓冲区中才返回，如果发送缓冲区中没有足够的空间容纳，那么就一直阻塞死等，特别的刚。</p><p>相比较而言<code>非阻塞写</code>的特点就比较佛系，当发送缓冲区中没有足够的空间容纳全部发送数据时，<code>非阻塞写</code>的特点是<code>能写多少写多少</code>，写不下了，就立即返回。并将写入到发送缓冲区的字节数返回给应用程序，方便用户线程不断的<code>轮训</code>尝试将<code>剩下的数据</code>写入发送缓冲区中。</p><h4 id="非阻塞IO模型"><a href="#非阻塞IO模型" class="headerlink" title="非阻塞IO模型"></a>非阻塞IO模型</h4><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504204257-35.png" alt="图片">非阻塞IO模型.png</p><p>基于以上<code>非阻塞IO</code>的特点，我们就不必像<code>阻塞IO</code>那样为每个请求分配一个线程去处理连接上的读写了。</p><p>我们可以利用<strong>一个线程或者很少的线程</strong>，去<code>不断地轮询</code>每个<code>Socket</code>的接收缓冲区是否有数据到达，如果没有数据，<code>不必阻塞</code>线程，而是接着去<code>轮询</code>下一个<code>Socket</code>接收缓冲区，直到轮询到数据后，处理连接上的读写，或者交给业务线程池去处理，轮询线程则<code>继续轮询</code>其他的<code>Socket</code>接收缓冲区。</p><p>这样一个<code>非阻塞IO模型</code>就实现了我们在本小节开始提出的需求：<strong>我们需要用尽可能少的线程去处理更多的连接</strong></p><h4 id="适用场景-1"><a href="#适用场景-1" class="headerlink" title="适用场景"></a>适用场景</h4><p>虽然<code>非阻塞IO模型</code>与<code>阻塞IO模型</code>相比，减少了很大一部分的资源消耗和系统开销。</p><p>但是它仍然有很大的性能问题，因为在<code>非阻塞IO模型</code>下，需要用户线程去<code>不断地</code>发起<code>系统调用</code>去轮训<code>Socket</code>接收缓冲区，这就需要用户线程不断地从<code>用户态</code>切换到<code>内核态</code>，<code>内核态</code>切换到<code>用户态</code>。随着并发量的增大，这个上下文切换的开销也是巨大的。</p><p>所以单纯的<code>非阻塞IO</code>模型还是无法适用于高并发的场景。只能适用于<code>C10K</code>以下的场景。</p><h2 id="9、IO多路复用"><a href="#9、IO多路复用" class="headerlink" title="9、IO多路复用"></a>9、IO多路复用</h2><p>在<code>非阻塞IO</code>这一小节的开头，我们提到<code>网络IO模型</code>的演变都是围绕着—<strong>如何用尽可能少的线程去处理更多的连接</strong>这个核心需求开始展开的。</p><p>本小节我们来谈谈<code>IO多路复用模型</code>，那么什么是<code>多路</code>？，什么又是<code>复用</code>呢？</p><p>我们还是以这个核心需求来对这两个概念展开阐述：</p><ul><li><strong>多路</strong>：我们的核心需求是要用尽可能少的线程来处理尽可能多的连接，这里的<code>多路</code>指的就是我们需要处理的众多连接。</li><li><strong>复用</strong>：核心需求要求我们使用<code>尽可能少的线程</code>，<code>尽可能少的系统开销</code>去处理<code>尽可能多</code>的连接（<code>多路</code>），那么这里的<code>复用</code>指的就是用<code>有限的资源</code>，比如用一个线程或者固定数量的线程去处理众多连接上的读写事件。换句话说，在<code>阻塞IO模型</code>中一个连接就需要分配一个独立的线程去专门处理这个连接上的读写，到了<code>IO多路复用模型</code>中，多个连接可以<code>复用</code>这一个独立的线程去处理这多个连接上的读写。</li></ul><p>好了，<code>IO多路复用模型</code>的概念解释清楚了，那么<strong>问题的关键</strong>是我们如何去实现这个<code>复用</code>，也就是如何让一个独立的线程去处理众多连接上的读写事件呢？</p><p>这个问题其实在<code>非阻塞IO模型</code>中已经给出了它的答案，在<code>非阻塞IO模型</code>中，利用<code>非阻塞</code>的系统IO调用去不断的轮询众多连接的<code>Socket</code>接收缓冲区看是否有数据到来，如果有则处理，如果没有则继续轮询下一个<code>Socket</code>。这样就达到了用一个线程去处理众多连接上的读写事件了。</p><p><strong>但是</strong><code>非阻塞IO模型</code>最大的问题就是需要不断的发起<code>系统调用</code>去轮询各个<code>Socket</code>中的接收缓冲区是否有数据到来，<code>频繁</code>的<code>系统调用</code>随之带来了大量的上下文切换开销。随着并发量的提升，这样也会导致非常严重的性能问题。</p><p><strong>那么如何避免频繁的系统调用同时又可以实现我们的核心需求呢？</strong></p><p>这就需要操作系统的内核来支持这样的操作，我们可以把频繁的轮询操作交给操作系统内核来替我们完成，这样就避免了在<code>用户空间</code>频繁的去使用系统调用来轮询所带来的性能开销。</p><p>正如我们所想，操作系统内核也确实为我们提供了这样的功能实现，下面我们来一起看下操作系统对<code>IO多路复用模型</code>的实现。</p><h3 id="select"><a href="#select" class="headerlink" title="select"></a>select</h3><p><code>select</code>是操作系统内核提供给我们使用的一个<code>系统调用</code>，它解决了在<code>非阻塞IO模型</code>中需要不断的发起<code>系统IO调用</code>去轮询<code>各个连接上的Socket</code>接收缓冲区所带来的<code>用户空间</code>与<code>内核空间</code>不断切换的<code>系统开销</code>。</p><p><code>select</code>系统调用将<code>轮询</code>的操作交给了<code>内核</code>来帮助我们完成，从而避免了在<code>用户空间</code>不断的发起轮询所带来的的系统性能开销。</p><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504298954-40.png" alt="图片">select.png</p><ul><li>首先用户线程在发起<code>select</code>系统调用的时候会<code>阻塞</code>在<code>select</code>系统调用上。此时，用户线程从<code>用户态</code>切换到了<code>内核态</code>完成了一次<code>上下文切换</code></li><li>用户线程将需要监听的<code>Socket</code>对应的文件描述符<code>fd</code>数组通过<code>select</code>系统调用传递给内核。此时，用户线程将<code>用户空间</code>中的文件描述符<code>fd</code>数组<code>拷贝</code>到<code>内核空间</code>。</li></ul><p>这里的<strong>文件描述符数组</strong>其实是一个<code>BitMap</code>，<code>BitMap</code>下标为<code>文件描述符fd</code>，下标对应的值为：<code>1</code>表示该<code>fd</code>上有读写事件，<code>0</code>表示该<code>fd</code>上没有读写事件。</p><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504298954-41.png" alt="图片">fd数组BitMap.png</p><p><strong>文件描述符fd</strong>其实就是一个<code>整数值</code>，在Linux中一切皆文件，<code>Socket</code>也是一个文件。描述进程所有信息的数据结构<code>task_struct</code>中有一个属性<code>struct files_struct *files</code>，它最终指向了一个数组，数组里存放了进程打开的所有文件列表，文件信息封装在<code>struct file</code>结构体中，这个数组存放的类型就是<code>struct file</code>结构体，<code>数组的下标</code>则是我们常说的文件描述符<code>fd</code>。</p><ul><li>当用户线程调用完<code>select</code>后开始进入<code>阻塞状态</code>，<code>内核</code>开始轮询遍历<code>fd</code>数组，查看<code>fd</code>对应的<code>Socket</code>接收缓冲区中是否有数据到来。如果有数据到来，则将<code>fd</code>对应<code>BitMap</code>的值设置为<code>1</code>。如果没有数据到来，则保持值为<code>0</code>。</li></ul><blockquote><p><strong>注意</strong>这里内核会修改原始的<code>fd</code>数组！！</p></blockquote><ul><li>内核遍历一遍<code>fd</code>数组后，如果发现有些<code>fd</code>上有IO数据到来，则将修改后的<code>fd</code>数组返回给用户线程。此时，会将<code>fd</code>数组从<code>内核空间</code>拷贝到<code>用户空间</code>。</li><li>当内核将修改后的<code>fd</code>数组返回给用户线程后，用户线程解除<code>阻塞</code>，由用户线程开始遍历<code>fd</code>数组然后找出<code>fd</code>数组中值为<code>1</code>的<code>Socket</code>文件描述符。最后对这些<code>Socket</code>发起系统调用读取数据。</li></ul><blockquote><p><code>select</code>不会告诉用户线程具体哪些<code>fd</code>上有IO数据到来，只是在<code>IO活跃</code>的<code>fd</code>上打上标记，将打好标记的完整<code>fd</code>数组返回给用户线程，所以用户线程还需要遍历<code>fd</code>数组找出具体哪些<code>fd</code>上有<code>IO数据</code>到来。</p></blockquote><ul><li>由于内核在遍历的过程中已经修改了<code>fd</code>数组，所以在用户线程遍历完<code>fd</code>数组后获取到<code>IO就绪</code>的<code>Socket</code>后，就需要<code>重置</code>fd数组，并重新调用<code>select</code>传入重置后的<code>fd</code>数组，让内核发起新的一轮遍历轮询。</li></ul><h4 id="API介绍"><a href="#API介绍" class="headerlink" title="API介绍"></a>API介绍</h4><p>当我们熟悉了<code>select</code>的原理后，就很容易理解内核给我们提供的<code>select API</code>了。</p><pre class="language-none"><code class="language-none">int select(int maxfdp1,fd_set *readset,fd_set *writeset,fd_set *exceptset,const struct timeval *timeout)</code></pre><p>从<code>select API</code>中我们可以看到，<code>select</code>系统调用是在规定的<code>超时时间内</code>，监听（<code>轮询</code>）用户感兴趣的文件描述符集合上的<code>可读</code>,<code>可写</code>,<code>异常</code>三类事件。</p><ul><li><code>maxfdp1 ：</code> select传递给内核监听的文件描述符集合中数值最大的文件描述符<code>+1</code>，目的是用于限定内核遍历范围。比如：<code>select</code>监听的文件描述符集合为<code>&#123;0,1,2,3,4&#125;</code>，那么<code>maxfdp1</code>的值为<code>5</code>。</li><li><code>fd_set *readset：</code> 对<code>可读事件</code>感兴趣的文件描述符集合。</li><li><code>fd_set *writeset：</code> 对<code>可写事件</code>感兴趣的文件描述符集合。</li><li><code>fd_set *exceptset：</code>对<code>异常事件</code>感兴趣的文件描述符集合。</li></ul><blockquote><p>这里的<code>fd_set</code>就是我们前边提到的<code>文件描述符数组</code>，是一个<code>BitMap</code>结构。</p></blockquote><ul><li><code>const struct timeval *timeout：</code>select系统调用超时时间，在这段时间内，内核如果没有发现有<code>IO就绪</code>的文件描述符，就直接返回。</li></ul><p>上小节提到，在<code>内核</code>遍历完<code>fd</code>数组后，发现有<code>IO就绪</code>的<code>fd</code>，则会将该<code>fd</code>对应的<code>BitMap</code>中的值设置为<code>1</code>，并将修改后的<code>fd</code>数组，返回给用户线程。</p><p>在用户线程中需要重新遍历<code>fd</code>数组，找出<code>IO就绪</code>的<code>fd</code>出来，然后发起真正的读写调用。</p><p>下面介绍下在用户线程中重新遍历<code>fd</code>数组的过程中，我们需要用到的<code>API</code>：</p><ul><li><code>void FD_ZERO(fd_set *fdset)：</code>清空指定的文件描述符集合，即让<code>fd_set</code>中不在包含任何文件描述符。</li><li><code>void FD_SET(int fd, fd_set *fdset)：</code>将一个给定的文件描述符加入集合之中。</li></ul><blockquote><p>每次调用<code>select</code>之前都要通过<code>FD_ZERO</code>和<code>FD_SET</code>重新设置文件描述符，因为文件描述符集合会在<code>内核</code>中<code>被修改</code>。</p></blockquote><ul><li><code>int FD_ISSET(int fd, fd_set *fdset)：</code>检查集合中指定的文件描述符是否可以读写。用户线程<code>遍历</code>文件描述符集合,调用该方法检查相应的文件描述符是否<code>IO就绪</code>。</li><li><code>void FD_CLR(int fd, fd_set *fdset)：</code>将一个给定的文件描述符从集合中删除</li></ul><h4 id="性能开销"><a href="#性能开销" class="headerlink" title="性能开销"></a>性能开销</h4><p>虽然<code>select</code>解决了<code>非阻塞IO模型</code>中频繁发起<code>系统调用</code>的问题，但是在整个<code>select</code>工作过程中，我们还是看出了<code>select</code>有些不足的地方。</p><ul><li>在发起<code>select</code>系统调用以及返回时，用户线程各发生了一次<code>用户态</code>到<code>内核态</code>以及<code>内核态</code>到<code>用户态</code>的上下文切换开销。<strong>发生2次上下文<code>切换</code></strong></li><li>在发起<code>select</code>系统调用以及返回时，用户线程在<code>内核态</code>需要将<code>文件描述符集合</code>从用户空间<code>拷贝</code>到内核空间。以及在内核修改完<code>文件描述符集合</code>后，又要将它从内核空间<code>拷贝</code>到用户空间。<strong>发生2次文件描述符集合的<code>拷贝</code></strong></li><li>虽然由原来在<code>用户空间</code>发起轮询<code>优化成了</code>在<code>内核空间</code>发起轮询但<code>select</code>不会告诉用户线程到底是哪些<code>Socket</code>上发生了<code>IO就绪</code>事件，只是对<code>IO就绪</code>的<code>Socket</code>作了标记，用户线程依然要<code>遍历</code>文件描述符集合去查找具体<code>IO就绪</code>的<code>Socket</code>。时间复杂度依然为<code>O(n)</code>。</li></ul><blockquote><p>大部分情况下，网络连接并不总是活跃的，如果<code>select</code>监听了大量的客户端连接，只有少数的连接活跃，然而使用轮询的这种方式会随着连接数的增大，效率会越来越低。</p></blockquote><ul><li><code>内核</code>会对原始的<code>文件描述符集合</code>进行修改。导致每次在用户空间重新发起<code>select</code>调用时，都需要对<code>文件描述符集合</code>进行<code>重置</code>。</li><li><code>BitMap</code>结构的文件描述符集合，长度为固定的<code>1024</code>,所以只能监听<code>0~1023</code>的文件描述符。</li><li><code>select</code>系统调用 不是线程安全的。</li></ul><p>以上<code>select</code>的不足所产生的<code>性能开销</code>都会随着并发量的增大而<code>线性增长</code>。</p><p>很明显<code>select</code>也不能解决<code>C10K</code>问题，只适用于<code>1000</code>个左右的并发连接场景。</p><h3 id="poll"><a href="#poll" class="headerlink" title="poll"></a>poll</h3><p><code>poll</code>相当于是改进版的<code>select</code>，但是工作原理基本和<code>select</code>没有本质的区别。</p><pre class="language-none"><code class="language-none">int poll(struct pollfd *fds, unsigned int nfds, int timeout)struct pollfd &#123;    int   fd;         &#x2F;* 文件描述符 *&#x2F;    short events;     &#x2F;* 需要监听的事件 *&#x2F;    short revents;    &#x2F;* 实际发生的事件 由内核修改设置 *&#x2F;&#125;;</code></pre><p><code>select</code>中使用的文件描述符集合是采用的固定长度为1024的<code>BitMap</code>结构的<code>fd_set</code>，而<code>poll</code>换成了一个<code>pollfd</code>结构没有固定长度的数组，这样就没有了最大描述符数量的限制（当然还会受到系统文件描述符限制）</p><p><code>poll</code>只是改进了<code>select</code>只能监听<code>1024</code>个文件描述符的数量限制，但是并没有在性能方面做出改进。和<code>select</code>上本质并没有多大差别。</p><ul><li>同样需要在<code>内核空间</code>和<code>用户空间</code>中对文件描述符集合进行<code>轮询</code>，查找出<code>IO就绪</code>的<code>Socket</code>的时间复杂度依然为<code>O(n)</code>。</li><li>同样需要将<code>包含大量文件描述符的集合</code>整体在<code>用户空间</code>和<code>内核空间</code>之间<code>来回复制</code>，<strong>无论这些文件描述符是否就绪</strong>。他们的开销都会随着文件描述符数量的增加而线性增大。</li><li><code>select，poll</code>在每次新增，删除需要监听的socket时，都需要将整个新的<code>socket</code>集合全量传至<code>内核</code>。</li></ul><p><code>poll</code>同样不适用高并发的场景。依然无法解决<code>C10K</code>问题。</p><h3 id="epoll"><a href="#epoll" class="headerlink" title="epoll"></a>epoll</h3><p>通过上边对<code>select,poll</code>核心原理的介绍，我们看到<code>select,poll</code>的性能瓶颈主要体现在下面三个地方：</p><ul><li>因为内核不会保存我们要监听的<code>socket</code>集合，所以在每次调用<code>select,poll</code>的时候都需要传入，传出全量的<code>socket</code>文件描述符集合。这导致了大量的文件描述符在<code>用户空间</code>和<code>内核空间</code>频繁的来回复制。</li><li>由于内核不会通知具体<code>IO就绪</code>的<code>socket</code>，只是在这些<code>IO就绪</code>的socket上打好标记，所以当<code>select</code>系统调用返回时，在<code>用户空间</code>还是需要<code>完整遍历</code>一遍<code>socket</code>文件描述符集合来获取具体<code>IO就绪</code>的<code>socket</code>。</li><li>在<code>内核空间</code>中也是通过遍历的方式来得到<code>IO就绪</code>的<code>socket</code>。</li></ul><p>下面我们来看下<code>epoll</code>是如何解决这些问题的。在介绍<code>epoll</code>的核心原理之前，我们需要介绍下理解<code>epoll</code>工作过程所需要的一些核心基础知识。</p><h4 id="Socket的创建"><a href="#Socket的创建" class="headerlink" title="Socket的创建"></a>Socket的创建</h4><p>服务端线程调用<code>accept</code>系统调用后开始<code>阻塞</code>，当有客户端连接上来并完成<code>TCP三次握手</code>后，<code>内核</code>会创建一个对应的<code>Socket</code>作为服务端与客户端通信的<code>内核</code>接口。</p><p>在Linux内核的角度看来，一切皆是文件，<code>Socket</code>也不例外，当内核创建出<code>Socket</code>之后，会将这个<code>Socket</code>放到当前进程所打开的文件列表中管理起来。</p><p>下面我们来看下进程管理这些打开的文件列表相关的内核数据结构是什么样的？在了解完这些数据结构后，我们会更加清晰的理解<code>Socket</code>在内核中所发挥的作用。并且对后面我们理解<code>epoll</code>的创建过程有很大的帮助。</p><h4 id="进程中管理文件列表结构"><a href="#进程中管理文件列表结构" class="headerlink" title="进程中管理文件列表结构"></a>进程中管理文件列表结构</h4><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504405952-46.png" alt="图片">进程中管理文件列表结构.png</p><p><code>struct tast_struct</code>是内核中用来表示进程的一个数据结构，它包含了进程的所有信息。本小节我们只列出和文件管理相关的属性。</p><p>其中进程内打开的所有文件是通过一个数组<code>fd_array</code>来进行组织管理，数组的下标即为我们常提到的<code>文件描述符</code>，数组中存放的是对应的文件数据结构<code>struct file</code>。每打开一个文件，内核都会创建一个<code>struct file</code>与之对应，并在<code>fd_array</code>中找到一个空闲位置分配给它，数组中对应的下标，就是我们在<code>用户空间</code>用到的<code>文件描述符</code>。</p><blockquote><p>对于任何一个进程，默认情况下，文件描述符 <code>0</code>表示 <code>stdin 标准输入</code>，文件描述符 <code>1</code>表示<code>stdout 标准输出</code>，文件描述符<code>2</code>表示<code>stderr 标准错误输出</code>。</p></blockquote><p>进程中打开的文件列表<code>fd_array</code>定义在内核数据结构<code>struct files_struct</code>中，在<code>struct fdtable</code>结构中有一个指针<code>struct fd **fd</code>指向<code>fd_array</code>。</p><p><strong>由于本小节讨论的是内核网络系统部分的数据结构</strong>，所以这里拿<code>Socket</code>文件类型来举例说明：</p><p>用于封装文件元信息的内核数据结构<code>struct file</code>中的<code>private_data</code>指针指向具体的<code>Socket</code>结构。</p><p><code>struct file</code>中的<code>file_operations</code>属性定义了文件的操作函数，不同的文件类型，对应的<code>file_operations</code>是不同的，针对<code>Socket</code>文件类型，这里的<code>file_operations</code>指向<code>socket_file_ops</code>。</p><blockquote><p>我们在<code>用户空间</code>对<code>Socket</code>发起的读写等系统调用，进入内核首先会调用的是<code>Socket</code>对应的<code>struct file</code>中指向的<code>socket_file_ops</code>。<strong>比如</strong>：对<code>Socket</code>发起<code>write</code>写操作，在内核中首先被调用的就是<code>socket_file_ops</code>中定义的<code>sock_write_iter</code>。<code>Socket</code>发起<code>read</code>读操作内核中对应的则是<code>sock_read_iter</code>。</p></blockquote><pre class="language-none"><code class="language-none">static const struct file_operations socket_file_ops &#x3D; &#123;  .owner &#x3D;  THIS_MODULE,  .llseek &#x3D;  no_llseek,  .read_iter &#x3D;  sock_read_iter,  .write_iter &#x3D;  sock_write_iter,  .poll &#x3D;    sock_poll,  .unlocked_ioctl &#x3D; sock_ioctl,  .mmap &#x3D;    sock_mmap,  .release &#x3D;  sock_close,  .fasync &#x3D;  sock_fasync,  .sendpage &#x3D;  sock_sendpage,  .splice_write &#x3D; generic_splice_sendpage,  .splice_read &#x3D;  sock_splice_read,&#125;;</code></pre><h4 id="Socket内核结构"><a href="#Socket内核结构" class="headerlink" title="Socket内核结构"></a>Socket内核结构</h4><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504405953-47.png" alt="图片">Socket内核结构.png</p><p>在我们进行网络程序的编写时会首先创建一个<code>Socket</code>，然后基于这个<code>Socket</code>进行<code>bind</code>，<code>listen</code>，我们先将这个<code>Socket</code>称作为<code>监听Socket</code>。</p><ol><li>当我们调用<code>accept</code>后，内核会基于<code>监听Socket</code>创建出来一个新的<code>Socket</code>专门用于与客户端之间的网络通信。并将<code>监听Socket</code>中的<code>Socket操作函数集合</code>（<code>inet_stream_ops</code>）<code>ops</code>赋值到新的<code>Socket</code>的<code>ops</code>属性中。</li></ol><pre class="language-none"><code class="language-none">const struct proto_ops inet_stream_ops &#x3D; &#123;  .bind &#x3D; inet_bind,  .connect &#x3D; inet_stream_connect,  .accept &#x3D; inet_accept,  .poll &#x3D; tcp_poll,  .listen &#x3D; inet_listen,  .sendmsg &#x3D; inet_sendmsg,  .recvmsg &#x3D; inet_recvmsg,  ......&#125;</code></pre><blockquote><p>这里需要注意的是，<code>监听的 socket</code>和真正用来网络通信的 <code>Socket</code>，是两个 Socket，一个叫作<code>监听 Socket</code>，一个叫作<code>已连接的Socket</code>。</p></blockquote><ol><li>接着内核会为<code>已连接的Socket</code>创建<code>struct file</code>并初始化，并把Socket文件操作函数集合（<code>socket_file_ops</code>）赋值给<code>struct file</code>中的<code>f_ops</code>指针。然后将<code>struct socket</code>中的<code>file</code>指针指向这个新分配申请的<code>struct file</code>结构体。</li></ol><blockquote><p>内核会维护两个队列：</p><ul><li>一个是已经完成<code>TCP三次握手</code>，连接状态处于<code>established</code>的连接队列。内核中为<code>icsk_accept_queue</code>。</li><li>一个是还没有完成<code>TCP三次握手</code>，连接状态处于<code>syn_rcvd</code>的半连接队列。</li></ul></blockquote><ol><li>然后调用<code>socket-&gt;ops-&gt;accept</code>，从<code>Socket内核结构图</code>中我们可以看到其实调用的是<code>inet_accept</code>，该函数会在<code>icsk_accept_queue</code>中查找是否有已经建立好的连接，如果有的话，直接从<code>icsk_accept_queue</code>中获取已经创建好的<code>struct sock</code>。并将这个<code>struct sock</code>对象赋值给<code>struct socket</code>中的<code>sock</code>指针。</li></ol><pre class="language-none"><code class="language-none">struct sock&#96;在&#96;struct socket&#96;中是一个非常核心的内核对象，正是在这里定义了我们在介绍&#96;网络包的接收发送流程&#96;中提到的&#96;接收队列&#96;，&#96;发送队列&#96;，&#96;等待队列&#96;，&#96;数据就绪回调函数指针&#96;，&#96;内核协议栈操作函数集合</code></pre><ul><li>根据创建<code>Socket</code>时发起的系统调用<code>sock_create</code>中的<code>protocol</code>参数(对于<code>TCP协议</code>这里的参数值为<code>SOCK_STREAM</code>)查找到对于 tcp 定义的操作方法实现集合 <code>inet_stream_ops</code> 和<code>tcp_prot</code>。并把它们分别设置到<code>socket-&gt;ops</code>和<code>sock-&gt;sk_prot</code>上。</li></ul><blockquote><p>这里可以回看下本小节开头的《Socket内核结构图》捋一下他们之间的关系。</p></blockquote><blockquote><p><code>socket</code>相关的操作接口定义在<code>inet_stream_ops</code>函数集合中，负责对上给用户提供接口。而<code>socket</code>与内核协议栈之间的操作接口定义在<code>struct sock</code>中的<code>sk_prot</code>指针上，这里指向<code>tcp_prot</code>协议操作函数集合。</p></blockquote><pre class="language-none"><code class="language-none">struct proto tcp_prot &#x3D; &#123;  .name      &#x3D; &quot;TCP&quot;,  .owner      &#x3D; THIS_MODULE,  .close      &#x3D; tcp_close,  .connect    &#x3D; tcp_v4_connect,  .disconnect    &#x3D; tcp_disconnect,  .accept      &#x3D; inet_csk_accept,  .keepalive    &#x3D; tcp_set_keepalive,  .recvmsg    &#x3D; tcp_recvmsg,  .sendmsg    &#x3D; tcp_sendmsg,  .backlog_rcv    &#x3D; tcp_v4_do_rcv,   ......&#125;</code></pre><blockquote><p>之前提到的对<code>Socket</code>发起的系统IO调用，在内核中首先会调用<code>Socket</code>的文件结构<code>struct file</code>中的<code>file_operations</code>文件操作集合，然后调用<code>struct socket</code>中的<code>ops</code>指向的<code>inet_stream_ops</code>socket操作函数，最终调用到<code>struct sock</code>中<code>sk_prot</code>指针指向的<code>tcp_prot</code>内核协议栈操作函数接口集合。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504405953-48.png" alt="图片">系统IO调用结构.png</p><ul><li>将<code>struct sock</code> 对象中的<code>sk_data_ready</code> 函数指针设置为 <code>sock_def_readable</code>，在<code>Socket</code>数据就绪的时候内核会回调该函数。</li><li><code>struct sock</code>中的<code>等待队列</code>中存放的是系统IO调用发生阻塞的<code>进程fd</code>，以及相应的<code>回调函数</code>。<strong>记住这个地方，后边介绍epoll的时候我们还会提到！</strong></li></ul><ol><li>当<code>struct file</code>，<code>struct socket</code>，<code>struct sock</code>这些核心的内核对象创建好之后，最后就是把<code>socket</code>对象对应的<code>struct file</code>放到进程打开的文件列表<code>fd_array</code>中。随后系统调用<code>accept</code>返回<code>socket</code>的文件描述符<code>fd</code>给用户程序。</li></ol><h4 id="阻塞IO中用户进程阻塞以及唤醒原理"><a href="#阻塞IO中用户进程阻塞以及唤醒原理" class="headerlink" title="阻塞IO中用户进程阻塞以及唤醒原理"></a>阻塞IO中用户进程阻塞以及唤醒原理</h4><p>在前边小节我们介绍<code>阻塞IO</code>的时候提到，当用户进程发起系统IO调用时，这里我们拿<code>read</code>举例，用户进程会在<code>内核态</code>查看对应<code>Socket</code>接收缓冲区是否有数据到来。</p><ul><li><code>Socket</code>接收缓冲区有数据，则拷贝数据到<code>用户空间</code>，系统调用返回。</li><li><code>Socket</code>接收缓冲区没有数据，则用户进程让出<code>CPU</code>进入<code>阻塞状态</code>，当数据到达接收缓冲区时，用户进程会被唤醒，从<code>阻塞状态</code>进入<code>就绪状态</code>，等待CPU调度。</li></ul><p>本小节我们就来看下用户进程是如何<code>阻塞</code>在<code>Socket</code>上，又是如何在<code>Socket</code>上被唤醒的。<strong>理解这个过程很重要，对我们理解epoll的事件通知过程很有帮助</strong></p><ul><li>首先我们在用户进程中对<code>Socket</code>进行<code>read</code>系统调用时，用户进程会从<code>用户态</code>转为<code>内核态</code>。</li><li>在进程的<code>struct task_struct</code>结构找到<code>fd_array</code>，并根据<code>Socket</code>的文件描述符<code>fd</code>找到对应的<code>struct file</code>，调用<code>struct file</code>中的文件操作函数结合<code>file_operations</code>，<code>read</code>系统调用对应的是<code>sock_read_iter</code>。</li><li>在<code>sock_read_iter</code>函数中找到<code>struct file</code>指向的<code>struct socket</code>，并调用<code>socket-&gt;ops-&gt;recvmsg</code>，这里我们知道调用的是<code>inet_stream_ops</code>集合中定义的<code>inet_recvmsg</code>。</li><li>在<code>inet_recvmsg</code>中会找到<code>struct sock</code>，并调用<code>sock-&gt;skprot-&gt;recvmsg</code>,这里调用的是<code>tcp_prot</code>集合中定义的<code>tcp_recvmsg</code>函数。</li></ul><blockquote><p>整个调用过程可以参考上边的《系统IO调用结构图》</p></blockquote><p><strong>熟悉了内核函数调用栈后，我们来看下系统IO调用在<code>tcp_recvmsg</code>内核函数中是如何将用户进程给阻塞掉的</strong></p><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504405953-49.png" alt="图片">系统IO调用阻塞原理.png</p><pre class="language-none"><code class="language-none">int tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,  size_t len, int nonblock, int flags, int *addr_len)&#123;    .................省略非核心代码...............   &#x2F;&#x2F;访问sock对象中定义的接收队列  skb_queue_walk(&amp;sk-&gt;sk_receive_queue, skb) &#123;    .................省略非核心代码...............  &#x2F;&#x2F;没有收到足够数据，调用sk_wait_data 阻塞当前进程  sk_wait_data(sk, &amp;timeo);&#125;int sk_wait_data(struct sock *sk, long *timeo)&#123; &#x2F;&#x2F;创建struct sock中等待队列上的元素wait_queue_t &#x2F;&#x2F;将进程描述符和回调函数autoremove_wake_function关联到wait_queue_t中 DEFINE_WAIT(wait); &#x2F;&#x2F; 调用 sk_sleep 获取 sock 对象下的等待队列的头指针wait_queue_head_t &#x2F;&#x2F; 调用prepare_to_wait将新创建的等待项wait_queue_t插入到等待队列中，并将进程状态设置为可打断 INTERRUPTIBLE prepare_to_wait(sk_sleep(sk), &amp;wait, TASK_INTERRUPTIBLE); set_bit(SOCK_ASYNC_WAITDATA, &amp;sk-&gt;sk_socket-&gt;flags); &#x2F;&#x2F; 通过调用schedule_timeout让出CPU，然后进行睡眠，导致一次上下文切换 rc &#x3D; sk_wait_event(sk, timeo, !skb_queue_empty(&amp;sk-&gt;sk_receive_queue)); ...</code></pre><ul><li>首先会在<code>DEFINE_WAIT</code>中创建<code>struct sock</code>中等待队列上的等待类型<code>wait_queue_t</code>。</li></ul><pre class="language-none"><code class="language-none">#define DEFINE_WAIT(name) DEFINE_WAIT_FUNC(name, autoremove_wake_function)#define DEFINE_WAIT_FUNC(name, function)    \ wait_queue_t name &#x3D; &#123;      \  .private &#x3D; current,    \  .func  &#x3D; function,    \  .task_list &#x3D; LIST_HEAD_INIT((name).task_list), \ &#125;</code></pre><p>等待类型<code>wait_queue_t</code>中的<code>private</code>用来关联<code>阻塞</code>在当前<code>socket</code>上的用户进程<code>fd</code>。<code>func</code>用来关联等待项上注册的回调函数。这里注册的是<code>autoremove_wake_function</code>。</p><ul><li>调用<code>sk_sleep(sk)</code>获取<code>struct sock</code>对象中的等待队列头指针<code>wait_queue_head_t</code>。</li><li>调用<code>prepare_to_wait</code>将新创建的等待项<code>wait_queue_t</code>插入到等待队列中，并将进程设置为可打断 <code>INTERRUPTIBL</code>。</li><li>调用<code>sk_wait_event</code>让出CPU，进程进入睡眠状态。</li></ul><p>用户进程的<code>阻塞过程</code>我们就介绍完了，关键是要理解记住<code>struct sock</code>中定义的等待队列上的等待类型<code>wait_queue_t</code>的结构。后面<code>epoll</code>的介绍中我们还会用到它。</p><p><strong>下面我们接着介绍当数据就绪后，用户进程是如何被唤醒的</strong></p><p>在本文开始介绍《网络包接收过程》这一小节中我们提到：</p><ul><li>当网络数据包到达网卡时，网卡通过<code>DMA</code>的方式将数据放到<code>RingBuffer</code>中。</li><li>然后向CPU发起硬中断，在硬中断响应程序中创建<code>sk_buffer</code>，并将网络数据拷贝至<code>sk_buffer</code>中。</li><li>随后发起软中断，内核线程<code>ksoftirqd</code>响应软中断，调用<code>poll函数</code>将<code>sk_buffer</code>送往内核协议栈做层层协议处理。</li><li>在传输层<code>tcp_rcv 函数</code>中，去掉TCP头，根据<code>四元组（源IP，源端口，目的IP，目的端口）</code>查找对应的<code>Socket</code>。</li><li>最后将<code>sk_buffer</code>放到<code>Socket</code>中的接收队列里。</li></ul><p>上边这些过程是内核接收网络数据的完整过程，下边我们来看下，当数据包接收完毕后，用户进程是如何被唤醒的。</p><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504405953-50.png" alt="图片">系统IO调用唤醒原理.png</p><ul><li>当软中断将<code>sk_buffer</code>放到<code>Socket</code>的接收队列上时，接着就会调用<code>数据就绪函数回调指针sk_data_ready</code>，前边我们提到，这个函数指针在初始化的时候指向了<code>sock_def_readable</code>函数。</li><li>在<code>sock_def_readable</code>函数中会去获取<code>socket-&gt;sock-&gt;sk_wq</code>等待队列。在<code>wake_up_common</code>函数中从等待队列<code>sk_wq</code>中找出<code>一个</code>等待项<code>wait_queue_t</code>，回调注册在该等待项上的<code>func</code>回调函数（<code>wait_queue_t-&gt;func</code>）,创建等待项<code>wait_queue_t</code>是我们提到，这里注册的回调函数是<code>autoremove_wake_function</code>。</li></ul><blockquote><p>即使是有多个进程都阻塞在同一个 socket 上，也只唤醒 1 个进程。其作用是为了避免惊群。</p></blockquote><ul><li>在<code>autoremove_wake_function</code>函数中，根据等待项<code>wait_queue_t</code>上的<code>private</code>关联的<code>阻塞进程fd</code>调用<code>try_to_wake_up</code>唤醒阻塞在该<code>Socket</code>上的进程。</li></ul><blockquote><p>记住<code>wait_queue_t</code>中的<code>func</code>函数指针，在<code>epoll</code>中这里会注册<code>epoll</code>的回调函数。</p></blockquote><p>现在理解<code>epoll</code>所需要的基础知识我们就介绍完了，唠叨了这么多，下面终于正式进入本小节的主题<code>epoll</code>了。</p><h4 id="epoll-create创建epoll对象"><a href="#epoll-create创建epoll对象" class="headerlink" title="epoll_create创建epoll对象"></a>epoll_create创建epoll对象</h4><p><code>epoll_create</code>是内核提供给我们创建<code>epoll</code>对象的一个系统调用，当我们在用户进程中调用<code>epoll_create</code>时，内核会为我们创建一个<code>struct eventpoll</code>对象，并且也有相应的<code>struct file</code>与之关联，同样需要把这个<code>struct eventpoll</code>对象所关联的<code>struct file</code>放入进程打开的文件列表<code>fd_array</code>中管理。</p><blockquote><p>熟悉了<code>Socket</code>的创建逻辑，<code>epoll</code>的创建逻辑也就不难理解了。</p></blockquote><blockquote><p><code>struct eventpoll</code>对象关联的<code>struct file</code>中的<code>file_operations 指针</code>指向的是<code>eventpoll_fops</code>操作函数集合。</p></blockquote><pre class="language-none"><code class="language-none">static const struct file_operations eventpoll_fops &#x3D; &#123;     .release &#x3D; ep_eventpoll_release;     .poll &#x3D; ep_eventpoll_poll,&#125;</code></pre><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504515900-61.png" alt="图片">eopll在进程中的整体结构.png</p><pre class="language-none"><code class="language-none">struct eventpoll &#123;    &#x2F;&#x2F;等待队列，阻塞在epoll上的进程会放在这里    wait_queue_head_t wq;    &#x2F;&#x2F;就绪队列，IO就绪的socket连接会放在这里    struct list_head rdllist;    &#x2F;&#x2F;红黑树用来管理所有监听的socket连接    struct rb_root rbr;    ......&#125;</code></pre><ul><li><code>wait_queue_head_t wq：</code>epoll中的等待队列，队列里存放的是<code>阻塞</code>在<code>epoll</code>上的用户进程。在<code>IO就绪</code>的时候<code>epoll</code>可以通过这个队列找到这些<code>阻塞</code>的进程并唤醒它们，从而执行<code>IO调用</code>读写<code>Socket</code>上的数据。</li></ul><blockquote><p>这里注意与<code>Socket</code>中的等待队列区分！！！</p></blockquote><ul><li><code>struct list_head rdllist：</code>epoll中的就绪队列，队列里存放的是都是<code>IO就绪</code>的<code>Socket</code>，被唤醒的用户进程可以直接读取这个队列获取<code>IO活跃</code>的<code>Socket</code>。无需再次遍历整个<code>Socket</code>集合。</li></ul><blockquote><p>这里正是<code>epoll</code>比<code>select ，poll</code>高效之处，<code>select ，poll</code>返回的是全部的<code>socket</code>连接，我们需要在<code>用户空间</code>再次遍历找出真正<code>IO活跃</code>的<code>Socket</code>连接。而<code>epoll</code>只是返回<code>IO活跃</code>的<code>Socket</code>连接。用户进程可以直接进行IO操作。</p></blockquote><ul><li><code>struct rb_root rbr :</code> 由于红黑树在<code>查找</code>，<code>插入</code>，<code>删除</code>等综合性能方面是最优的，所以epoll内部使用一颗红黑树来管理海量的<code>Socket</code>连接。</li></ul><blockquote><p><code>select</code>用<code>数组</code>管理连接，<code>poll</code>用<code>链表</code>管理连接。</p></blockquote><h4 id="epoll-ctl向epoll对象中添加监听的Socket"><a href="#epoll-ctl向epoll对象中添加监听的Socket" class="headerlink" title="epoll_ctl向epoll对象中添加监听的Socket"></a>epoll_ctl向epoll对象中添加监听的Socket</h4><p>当我们调用<code>epoll_create</code>在内核中创建出<code>epoll</code>对象<code>struct eventpoll</code>后，我们就可以利用<code>epoll_ctl</code>向<code>epoll</code>中添加我们需要管理的<code>Socket</code>连接了。</p><ol><li>首先要在epoll内核中创建一个表示<code>Socket连接</code>的数据结构<code>struct epitem</code>，而在<code>epoll</code>中为了综合性能的考虑，采用一颗红黑树来管理这些海量<code>socket连接</code>。所以<code>struct epitem</code>是一个红黑树节点。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504515900-62.png" alt="图片">struct epitem.png</p><pre class="language-none"><code class="language-none">struct epitem&#123;      &#x2F;&#x2F;指向所属epoll对象      struct eventpoll *ep;       &#x2F;&#x2F;注册的感兴趣的事件,也就是用户空间的epoll_event           struct epoll_event event;       &#x2F;&#x2F;指向epoll对象中的就绪队列      struct list_head rdllink;        &#x2F;&#x2F;指向epoll中对应的红黑树节点      struct rb_node rbn;           &#x2F;&#x2F;指向epitem所表示的socket-&gt;file结构以及对应的fd      struct epoll_filefd ffd;                    &#125;</code></pre><blockquote><p>这里重点记住<code>struct epitem</code>结构中的<code>rdllink</code>以及<code>epoll_filefd</code>成员，后面我们会用到。</p></blockquote><ol><li>在内核中创建完表示<code>Socket连接</code>的数据结构<code>struct epitem</code>后，我们就需要在<code>Socket</code>中的等待队列上创建等待项<code>wait_queue_t</code>并且注册<code>epoll的回调函数ep_poll_callback</code>。</li></ol><p>通过<code>《阻塞IO中用户进程阻塞以及唤醒原理》</code>小节的铺垫，我想大家已经猜到这一步的意义所在了吧！当时在等待项<code>wait_queue_t</code>中注册的是<code>autoremove_wake_function</code>回调函数。还记得吗？</p><blockquote><p>epoll的回调函数<code>ep_poll_callback</code>正是<code>epoll</code>同步IO事件通知机制的核心所在，也是区别于<code>select，poll</code>采用内核轮询方式的根本性能差异所在。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504515900-63.png" alt="图片">epitem创建等待项.png</p><p><strong>这里又出现了一个新的数据结构<code>struct eppoll_entry</code>，那它的作用是干什么的呢？大家可以结合上图先猜测下它的作用!</strong></p><p>我们知道<code>socket-&gt;sock-&gt;sk_wq</code>等待队列中的类型是<code>wait_queue_t</code>，我们需要在<code>struct epitem</code>所表示的<code>socket</code>的等待队列上注册<code>epoll</code>回调函数<code>ep_poll_callback</code>。</p><p>这样当数据到达<code>socket</code>中的接收队列时，内核会回调<code>sk_data_ready</code>，在<code>阻塞IO中用户进程阻塞以及唤醒原理</code>这一小节中，我们知道这个<code>sk_data_ready</code>函数指针会指向<code>sk_def_readable</code>函数，在<code>sk_def_readable</code>中会回调注册在等待队列里的等待项<code>wait_queue_t -&gt; func</code>回调函数<code>ep_poll_callback</code>。**在<code>ep_poll_callback</code>中需要找到<code>epitem</code>**，将<code>IO就绪</code>的<code>epitem</code>放入<code>epoll</code>中的就绪队列中。</p><p>而<code>socket</code>等待队列中类型是<code>wait_queue_t</code>无法关联到<code>epitem</code>。所以就出现了<code>struct eppoll_entry</code>结构体，它的作用就是关联<code>Socket</code>等待队列中的等待项<code>wait_queue_t</code>和<code>epitem</code>。</p><pre class="language-none"><code class="language-none">struct eppoll_entry &#123;    &#x2F;&#x2F;指向关联的epitem   struct epitem *base;   &#x2F;&#x2F; 关联监听socket中等待队列中的等待项 (private &#x3D; null  func &#x3D; ep_poll_callback)   wait_queue_t wait;      &#x2F;&#x2F; 监听socket中等待队列头指针   wait_queue_head_t *whead;     .........  &#125;; </code></pre><p>这样在<code>ep_poll_callback</code>回调函数中就可以根据<code>Socket</code>等待队列中的等待项<code>wait</code>，通过<code>container_of宏</code>找到<code>eppoll_entry</code>，继而找到<code>epitem</code>了。</p><blockquote><p><code>container_of</code>在Linux内核中是一个常用的宏，用于从包含在某个结构中的指针获得结构本身的指针，通俗地讲就是通过结构体变量中某个成员的首地址进而获得整个结构体变量的首地址。</p></blockquote><blockquote><p>这里需要注意下这次等待项<code>wait_queue_t</code>中的<code>private</code>设置的是<code>null</code>，因为这里<code>Socket</code>是交给<code>epoll</code>来管理的，阻塞在<code>Socket</code>上的进程是也由<code>epoll</code>来唤醒。在等待项<code>wait_queue_t</code>注册的<code>func</code>是<code>ep_poll_callback</code>而不是<code>autoremove_wake_function</code>，<code>阻塞进程</code>并不需要<code>autoremove_wake_function</code>来唤醒，所以这里设置<code>private</code>为<code>null</code></p></blockquote><ol><li>当在<code>Socket</code>的等待队列中创建好等待项<code>wait_queue_t</code>并且注册了<code>epoll</code>的回调函数<code>ep_poll_callback</code>，然后又通过<code>eppoll_entry</code>关联了<code>epitem</code>后。剩下要做的就是将<code>epitem</code>插入到<code>epoll</code>中的红黑树<code>struct rb_root rbr</code>中。</li></ol><blockquote><p>这里可以看到<code>epoll</code>另一个优化的地方，<code>epoll</code>将所有的<code>socket</code>连接通过内核中的红黑树来集中管理。每次添加或者删除<code>socket连接</code>都是增量添加删除，而不是像<code>select，poll</code>那样每次调用都是全量<code>socket连接</code>集合传入内核。避免了<code>频繁大量</code>的<code>内存拷贝</code>。</p></blockquote><h4 id="epoll-wait同步阻塞获取IO就绪的Socket"><a href="#epoll-wait同步阻塞获取IO就绪的Socket" class="headerlink" title="epoll_wait同步阻塞获取IO就绪的Socket"></a>epoll_wait同步阻塞获取IO就绪的Socket</h4><ol><li>用户程序调用<code>epoll_wait</code>后，内核首先会查找epoll中的就绪队列<code>eventpoll-&gt;rdllist</code>是否有<code>IO就绪</code>的<code>epitem</code>。<code>epitem</code>里封装了<code>socket</code>的信息。如果就绪队列中有就绪的<code>epitem</code>，就将<code>就绪的socket</code>信息封装到<code>epoll_event</code>返回。</li><li>如果<code>eventpoll-&gt;rdllist</code>就绪队列中没有<code>IO就绪</code>的<code>epitem</code>，则会创建等待项<code>wait_queue_t</code>，将用户进程的<code>fd</code>关联到<code>wait_queue_t-&gt;private</code>上，并在等待项<code>wait_queue_t-&gt;func</code>上注册回调函数<code>default_wake_function</code>。最后将等待项添加到<code>epoll</code>中的等待队列中。用户进程让出CPU，进入<code>阻塞状态</code>。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504515900-64.png" alt="图片">          epoll_wait同步获取数据.png</p><blockquote><p>这里和<code>阻塞IO模型</code>中的阻塞原理是一样的，只不过在<code>阻塞IO模型</code>中注册到等待项<code>wait_queue_t-&gt;func</code>上的是<code>autoremove_wake_function</code>，并将等待项添加到<code>socket</code>中的等待队列中。这里注册的是<code>default_wake_function</code>，将等待项添加到<code>epoll</code>中的等待队列上。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504515900-65.png" alt="图片">数据到来epoll_wait流程.png</p><ol><li><strong>前边做了那么多的知识铺垫，下面终于到了<code>epoll</code>的整个工作流程了：</strong></li></ol><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504515900-66.png" alt="图片">epoll_wait处理过程.png</p><ul><li>当网络数据包在软中断中经过内核协议栈的处理到达<code>socket</code>的接收缓冲区时，紧接着会调用socket的数据就绪回调指针<code>sk_data_ready</code>，回调函数为<code>sock_def_readable</code>。在<code>socket</code>的等待队列中找出等待项，其中等待项中注册的回调函数为<code>ep_poll_callback</code>。</li><li>在回调函数<code>ep_poll_callback</code>中，根据<code>struct eppoll_entry</code>中的<code>struct wait_queue_t wait</code>通过<code>container_of宏</code>找到<code>eppoll_entry</code>对象并通过它的<code>base</code>指针找到封装<code>socket</code>的数据结构<code>struct epitem</code>，并将它加入到<code>epoll</code>中的就绪队列<code>rdllist</code>中。</li><li>随后查看<code>epoll</code>中的等待队列中是否有等待项，也就是说查看是否有进程阻塞在<code>epoll_wait</code>上等待<code>IO就绪</code>的<code>socket</code>。如果没有等待项，则软中断处理完成。</li><li>如果有等待项，则回到注册在等待项中的回调函数<code>default_wake_function</code>,在回调函数中唤醒<code>阻塞进程</code>，并将就绪队列<code>rdllist</code>中的<code>epitem</code>的<code>IO就绪</code>socket信息封装到<code>struct epoll_event</code>中返回。</li><li>用户进程拿到<code>epoll_event</code>获取<code>IO就绪</code>的socket，发起系统IO调用读取数据。</li></ul><h2 id="10、再谈水平触发和边缘触发"><a href="#10、再谈水平触发和边缘触发" class="headerlink" title="10、再谈水平触发和边缘触发"></a>10、再谈水平触发和边缘触发</h2><p>网上有大量的关于这两种模式的讲解，大部分讲的比较模糊，感觉只是强行从概念上进行描述，看完让人难以理解。所以在这里，笔者想结合上边<code>epoll</code>的工作过程，再次对这两种模式做下自己的解读，力求清晰的解释出这两种工作模式的异同。</p><p>经过上边对<code>epoll</code>工作过程的详细解读，我们知道，当我们监听的<code>socket</code>上有数据到来时，软中断会执行<code>epoll</code>的回调函数<code>ep_poll_callback</code>,在回调函数中会将<code>epoll</code>中描述<code>socket信息</code>的数据结构<code>epitem</code>插入到<code>epoll</code>中的就绪队列<code>rdllist</code>中。随后用户进程从<code>epoll</code>的等待队列中被唤醒，<code>epoll_wait</code>将<code>IO就绪</code>的<code>socket</code>返回给用户进程，随即<code>epoll_wait</code>会清空<code>rdllist</code>。</p><p><strong>水平触发</strong>和<strong>边缘触发</strong>最关键的<strong>区别</strong>就在于当<code>socket</code>中的接收缓冲区还有数据可读时。**<code>epoll_wait</code>是否会清空<code>rdllist</code>。**</p><ul><li><strong>水平触发</strong>：在这种模式下，用户线程调用<code>epoll_wait</code>获取到<code>IO就绪</code>的socket后，对<code>Socket</code>进行系统IO调用读取数据，假设<code>socket</code>中的数据只读了一部分没有全部读完，这时再次调用<code>epoll_wait</code>，<code>epoll_wait</code>会检查这些<code>Socket</code>中的接收缓冲区是否还有数据可读，如果还有数据可读，就将<code>socket</code>重新放回<code>rdllist</code>。所以当<code>socket</code>上的IO没有被处理完时，再次调用<code>epoll_wait</code>依然可以获得这些<code>socket</code>，用户进程可以接着处理<code>socket</code>上的IO事件。</li><li><strong>边缘触发：</strong> 在这种模式下，<code>epoll_wait</code>就会直接清空<code>rdllist</code>，不管<code>socket</code>上是否还有数据可读。所以在边缘触发模式下，当你没有来得及处理<code>socket</code>接收缓冲区的剩下可读数据时，再次调用<code>epoll_wait</code>，因为这时<code>rdlist</code>已经被清空了，<code>socket</code>不会再次从<code>epoll_wait</code>中返回，所以用户进程就不会再次获得这个<code>socket</code>了，也就无法在对它进行IO处理了。<strong>除非，这个<code>socket</code>上有新的IO数据到达</strong>，根据<code>epoll</code>的工作过程，该<code>socket</code>会被再次放入<code>rdllist</code>中。</li></ul><blockquote><p>如果你在<code>边缘触发模式</code>下，处理了部分<code>socket</code>上的数据，那么想要处理剩下部分的数据，就只能等到这个<code>socket</code>上再次有网络数据到达。</p></blockquote><p>在<code>Netty</code>中实现的<code>EpollSocketChannel</code>默认的就是<code>边缘触发</code>模式。<code>JDK</code>的<code>NIO</code>默认是<code>水平触发</code>模式。</p><h3 id="epoll对select，poll的优化总结"><a href="#epoll对select，poll的优化总结" class="headerlink" title="epoll对select，poll的优化总结"></a>epoll对select，poll的优化总结</h3><ul><li><code>epoll</code>在内核中通过<code>红黑树</code>管理海量的连接，所以在调用<code>epoll_wait</code>获取<code>IO就绪</code>的socket时，不需要传入监听的socket文件描述符。从而避免了海量的文件描述符集合在<code>用户空间</code>和<code>内核空间</code>中来回复制。</li></ul><blockquote><p><code>select，poll</code>每次调用时都需要传递全量的文件描述符集合，导致大量频繁的拷贝操作。</p></blockquote><ul><li><code>epoll</code>仅会通知<code>IO就绪</code>的socket。避免了在用户空间遍历的开销。</li></ul><blockquote><p><code>select，poll</code>只会在<code>IO就绪</code>的socket上打好标记，依然是全量返回，所以在用户空间还需要用户程序在一次遍历全量集合找出具体<code>IO就绪</code>的socket。</p></blockquote><ul><li><code>epoll</code>通过在<code>socket</code>的等待队列上注册回调函数<code>ep_poll_callback</code>通知用户程序<code>IO就绪</code>的socket。避免了在内核中轮询的开销。</li></ul><blockquote><p>大部分情况下<code>socket</code>上并不总是<code>IO活跃</code>的，在面对海量连接的情况下，<code>select，poll</code>采用内核轮询的方式获取<code>IO活跃</code>的socket，无疑是性能低下的核心原因。</p></blockquote><p>根据以上<code>epoll</code>的性能优势，它是目前为止各大主流网络框架，以及反向代理中间件使用到的网络IO模型。</p><p>利用<code>epoll</code>多路复用IO模型可以轻松的解决<code>C10K</code>问题。</p><p><code>C100k</code>的解决方案也还是基于<code>C10K</code>的方案，通过<code>epoll</code> 配合线程池，再加上 CPU、内存和网络接口的性能和容量提升。大部分情况下，<code>C100K</code>很自然就可以达到。</p><p>甚至<code>C1000K</code>的解决方法，本质上还是构建在 <code>epoll</code> 的<code>多路复用 I/O 模型</code>上。只不过，除了 I&#x2F;O 模型之外，还需要从应用程序到 Linux 内核、再到 CPU、内存和网络等各个层次的深度优化，特别是需要借助硬件，来卸载那些原来通过软件处理的大量功能（<code>去掉大量的中断响应开销</code>，<code>以及内核协议栈处理的开销</code>）。</p><h2 id="11、信号驱动IO"><a href="#11、信号驱动IO" class="headerlink" title="11、信号驱动IO"></a>11、信号驱动IO</h2><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504628512-79.png" alt="图片">信号驱动IO.png</p><p>大家对这个装备肯定不会陌生，当我们去一些美食城吃饭的时候，点完餐付了钱，老板会给我们一个信号器。然后我们带着这个信号器可以去找餐桌，或者干些其他的事情。当信号器亮了的时候，这时代表饭餐已经做好，我们可以去窗口取餐了。</p><p>这个典型的生活场景和我们要介绍的<code>信号驱动IO模型</code>就很像。</p><p>在<code>信号驱动IO模型</code>下，用户进程操作通过<code>系统调用 sigaction 函数</code>发起一个 IO 请求，在对应的<code>socket</code>注册一个<code>信号回调</code>，此时<code>不阻塞</code>用户进程，进程会继续工作。当内核数据就绪时，内核就为该进程生成一个 <code>SIGIO 信号</code>，通过信号回调通知进程进行相关 IO 操作。</p><blockquote><p>这里需要注意的是：<code>信号驱动式 IO 模型</code>依然是<code>同步IO</code>，因为它虽然可以在等待数据的时候不被阻塞，也不会频繁的轮询，但是当数据就绪，内核信号通知后，用户进程依然要自己去读取数据，在<code>数据拷贝阶段</code>发生阻塞。</p></blockquote><blockquote><p>信号驱动 IO模型 相比于前三种 IO 模型，实现了在等待数据就绪时，进程不被阻塞，主循环可以继续工作，所以<code>理论上</code>性能更佳。</p></blockquote><p>但是实际上，使用<code>TCP协议</code>通信时，<code>信号驱动IO模型</code>几乎<code>不会被采用</code>。原因如下：</p><ul><li>信号IO 在大量 IO 操作时可能会因为信号队列溢出导致没法通知</li><li><code>SIGIO 信号</code>是一种 Unix 信号，信号没有附加信息，如果一个信号源有多种产生信号的原因，信号接收者就无法确定究竟发生了什么。而 TCP socket 生产的信号事件有七种之多，这样应用程序收到 SIGIO，根本无从区分处理。</li></ul><p>但<code>信号驱动IO模型</code>可以用在 <code>UDP</code>通信上，因为UDP 只有<code>一个数据请求事件</code>，这也就意味着在正常情况下 UDP 进程只要捕获 SIGIO 信号，就调用 <code>read 系统调用</code>读取到达的数据。如果出现异常，就返回一个异常错误。</p><hr><p>这里插句题外话，大家觉不觉得<code>阻塞IO模型</code>在生活中的例子就像是我们在食堂排队打饭。你自己需要排队去打饭同时打饭师傅在配菜的过程中你需要等待。</p><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504628512-80.png" alt="图片">阻塞IO.png</p><p><code>IO多路复用模型</code>就像是我们在饭店门口排队等待叫号。叫号器就好比<code>select,poll,epoll</code>可以统一管理全部顾客的<code>吃饭就绪</code>事件，客户好比是<code>socket</code>连接，谁可以去吃饭了，叫号器就通知谁。</p><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504628512-81.png" alt="图片"></p><p>IO多路复用.png</p><p>##异步IO（AIO）</p><p>以上介绍的四种<code>IO模型</code>均为<code>同步IO</code>，它们都会阻塞在第二阶段<code>数据拷贝阶段</code>。</p><p>通过在前边小节《同步与异步》中的介绍，相信大家很容易就会理解<code>异步IO模型</code>，在<code>异步IO模型</code>下，IO操作在<code>数据准备阶段</code>和<code>数据拷贝阶段</code>均是由内核来完成，不会对应用程序造成任何阻塞。应用进程只需要在<code>指定的数组</code>中引用数据即可。</p><p><code>异步 IO</code> 与<code>信号驱动 IO</code> 的主要区别在于：<code>信号驱动 IO</code> 由内核通知何时可以<code>开始一个 IO 操作</code>，而<code>异步 IO</code>由内核通知 <code>IO 操作何时已经完成</code>。</p><p>举个生活中的例子：<code>异步IO模型</code>就像我们去一个高档饭店里的包间吃饭，我们只需要坐在包间里面，点完餐（<code>类比异步IO调用</code>）之后，我们就什么也不需要管，该喝酒喝酒，该聊天聊天，饭餐做好后服务员（<code>类比内核</code>）会自己给我们送到包间（<code>类比用户空间</code>）来。整个过程没有任何阻塞。</p><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504628512-82.png" alt="图片"></p><p>异步IO.png</p><p><code>异步IO</code>的系统调用需要操作系统内核来支持，目前只有<code>Window</code>中的<code>IOCP</code>实现了非常成熟的<code>异步IO机制</code>。</p><p>而<code>Linux</code>系统对<code>异步IO机制</code>实现的不够成熟，且与<code>NIO</code>的性能相比提升也不明显。</p><blockquote><p>但Linux kernel 在5.1版本由Facebook的大神Jens Axboe引入了新的异步IO库<code>io_uring</code> 改善了原来Linux native AIO的一些性能问题。性能相比<code>Epoll</code>以及之前原生的<code>AIO</code>提高了不少，值得关注。</p></blockquote><p>再加上<code>信号驱动IO模型</code>不适用<code>TCP协议</code>，所以目前大部分采用的还是<code>IO多路复用模型</code>。</p><h2 id="12、IO线程模型"><a href="#12、IO线程模型" class="headerlink" title="12、IO线程模型"></a>12、IO线程模型</h2><p>在前边内容的介绍中，我们详述了网络数据包的接收和发送过程，并通过介绍5种<code>IO模型</code>了解了内核是如何读取网络数据并通知给用户线程的。</p><p>前边的内容都是以<code>内核空间</code>的视角来剖析网络数据的收发模型，本小节我们站在<code>用户空间</code>的视角来看下如果对网络数据进行收发。</p><p>相对<code>内核</code>来讲，<code>用户空间的IO线程模型</code>相对就简单一些。这些<code>用户空间</code>的<code>IO线程模型</code>都是在讨论当多线程一起配合工作时谁负责接收连接，谁负责响应IO 读写、谁负责计算、谁负责发送和接收，仅仅是用户IO线程的不同分工模式罢了。</p><h3 id="Reactor"><a href="#Reactor" class="headerlink" title="Reactor"></a>Reactor</h3><p><code>Reactor</code>是利用<code>NIO</code>对<code>IO线程</code>进行不同的分工：</p><ul><li>使用前边我们提到的<code>IO多路复用模型</code>比如<code>select,poll,epoll,kqueue</code>,进行IO事件的注册和监听。</li><li>将监听到<code>就绪的IO事件</code>分发<code>dispatch</code>到各个具体的处理<code>Handler</code>中进行相应的<code>IO事件处理</code>。</li></ul><p>通过<code>IO多路复用技术</code>就可以不断的监听<code>IO事件</code>，不断的分发<code>dispatch</code>，就像一个<code>反应堆</code>一样，看起来像不断的产生<code>IO事件</code>，因此我们称这种模式为<code>Reactor</code>模型。</p><p>下面我们来看下<code>Reactor模型</code>的三种分类：</p><h4 id="单Reactor单线程"><a href="#单Reactor单线程" class="headerlink" title="单Reactor单线程"></a>单Reactor单线程</h4><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504628512-83.png" alt="图片">单Reactor单线程</p><p><code>Reactor模型</code>是依赖<code>IO多路复用技术</code>实现监听<code>IO事件</code>，从而源源不断的产生<code>IO就绪事件</code>，在Linux系统下我们使用<code>epoll</code>来进行<code>IO多路复用</code>，我们以Linux系统为例：</p><ul><li>单<code>Reactor</code>意味着只有一个<code>epoll</code>对象，用来监听所有的事件，比如<code>连接事件</code>，<code>读写事件</code>。</li><li><code>单线程</code>意味着只有一个线程来执行<code>epoll_wait</code>获取<code>IO就绪</code>的<code>Socket</code>，然后对这些就绪的<code>Socket</code>执行读写，以及后边的业务处理也依然是这个线程。</li></ul><p><code>单Reactor单线程</code>模型就好比我们开了一个很小很小的小饭馆，作为老板的我们需要一个人干所有的事情，包括：迎接顾客（<code>accept事件</code>），为顾客介绍菜单等待顾客点菜(<code>IO请求</code>)，做菜（<code>业务处理</code>），上菜（<code>IO响应</code>），送客（<code>断开连接</code>）。</p><h4 id="单Reactor多线程"><a href="#单Reactor多线程" class="headerlink" title="单Reactor多线程"></a>单Reactor多线程</h4><p>随着客人的增多（<code>并发请求</code>），显然饭馆里的事情只有我们一个人干（<code>单线程</code>）肯定是忙不过来的，这时候我们就需要多招聘一些员工（<code>多线程</code>）来帮着一起干上述的事情。</p><p>于是就有了<code>单Reactor多线程</code>模型：</p><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504628512-84.png" alt="图片" style="zoom:80%;" />单Reactor多线程</p><ul><li>这种模式下，也是只有一个<code>epoll</code>对象来监听所有的<code>IO事件</code>，一个线程来调用<code>epoll_wait</code>获取<code>IO就绪</code>的<code>Socket</code>。</li><li>但是当<code>IO就绪事件</code>产生时，这些<code>IO事件</code>对应处理的业务<code>Handler</code>，我们是通过线程池来执行。这样相比<code>单Reactor单线程</code>模型提高了执行效率，充分发挥了多核CPU的优势。</li></ul><h4 id="主从Reactor多线程"><a href="#主从Reactor多线程" class="headerlink" title="主从Reactor多线程"></a>主从Reactor多线程</h4><p>做任何事情都要区分<code>事情的优先级</code>，我们应该<code>优先高效</code>的去做<code>优先级更高</code>的事情，而不是一股脑不分优先级的全部去做。</p><p>当我们的小饭馆客人越来越多（<code>并发量越来越大</code>），我们就需要扩大饭店的规模，在这个过程中我们发现，<code>迎接客人</code>是饭店最重要的工作，我们要先把客人迎接进来，不能让客人一看人多就走掉，只要客人进来了，哪怕菜做的慢一点也没关系。</p><p>于是，<code>主从Reactor多线程</code>模型就产生了：</p><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504628512-85.png" alt="图片">主从Reactor多线程</p><ul><li>我们由原来的<code>单Reactor</code>变为了<code>多Reactor</code>。<code>主Reactor</code>用来优先<code>专门</code>做优先级最高的事情，也就是迎接客人（<code>处理连接事件</code>），对应的处理<code>Handler</code>就是图中的<code>acceptor</code>。</li><li>当创建好连接，建立好对应的<code>socket</code>后，在<code>acceptor</code>中将要监听的<code>read事件</code>注册到<code>从Reactor</code>中，由<code>从Reactor</code>来监听<code>socket</code>上的<code>读写</code>事件。</li><li>最终将读写的业务逻辑处理交给线程池处理。</li></ul><blockquote><p><strong>注意</strong>：这里向<code>从Reactor</code>注册的只是<code>read事件</code>，并没有注册<code>write事件</code>，因为<code>read事件</code>是由<code>epoll内核</code>触发的，而<code>write事件</code>则是由用户业务线程触发的（<code>什么时候发送数据是由具体业务线程决定的</code>），所以<code>write事件</code>理应是由<code>用户业务线程</code>去注册。</p></blockquote><blockquote><p>用户线程注册<code>write事件</code>的时机是只有当用户发送的数据<code>无法一次性</code>全部写入<code>buffer</code>时，才会去注册<code>write事件</code>，等待<code>buffer重新可写</code>时，继续写入剩下的发送数据、如果用户线程可以一股脑的将发送数据全部写入<code>buffer</code>，那么也就无需注册<code>write事件</code>到<code>从Reactor</code>中。</p></blockquote><p><code>主从Reactor多线程</code>模型是现在大部分主流网络框架中采用的一种<code>IO线程模型</code>。我们本系列的主题<code>Netty</code>就是用的这种模型。</p><h3 id="Proactor"><a href="#Proactor" class="headerlink" title="Proactor"></a>Proactor</h3><p><code>Proactor</code>是基于<code>AIO</code>对<code>IO线程</code>进行分工的一种模型。前边我们介绍了<code>异步IO模型</code>，它是操作系统内核支持的一种全异步编程模型，在<code>数据准备阶段</code>和<code>数据拷贝阶段</code>全程无阻塞。</p><p><code>ProactorIO线程模型</code>将<code>IO事件的监听</code>，<code>IO操作的执行</code>，<code>IO结果的dispatch</code>统统交给<code>内核</code>来做。</p><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504704761-100.png" alt="图片">proactor.png</p><p><strong><code>Proactor模型</code>组件介绍：</strong></p><ul><li><code>completion handler</code> 为用户程序定义的异步IO操作回调函数，在异步IO操作完成时会被内核回调并通知IO结果。</li><li><code>Completion Event Queue</code> 异步IO操作完成后，会产生对应的<code>IO完成事件</code>，将<code>IO完成事件</code>放入该队列中。</li><li><code>Asynchronous Operation Processor</code> 负责<code>异步IO</code>的执行。执行完成后产生<code>IO完成事件</code>放入<code>Completion Event Queue</code> 队列中。</li><li><code>Proactor</code> 是一个事件循环派发器，负责从<code>Completion Event Queue</code>中获取<code>IO完成事件</code>，并回调与<code>IO完成事件</code>关联的<code>completion handler</code>。</li><li><code>Initiator</code> 初始化异步操作（<code>asynchronous operation</code>）并通过<code>Asynchronous Operation Processor</code>将<code>completion handler</code>和<code>proactor</code>注册到内核。</li></ul><p><strong><code>Proactor模型</code>执行过程：</strong></p><ul><li>用户线程发起<code>aio_read</code>，并告诉<code>内核</code>用户空间中的读缓冲区地址，以便<code>内核</code>完成<code>IO操作</code>将结果放入<code>用户空间</code>的读缓冲区，用户线程直接可以读取结果（<code>无任何阻塞</code>）。</li><li><code>Initiator</code> 初始化<code>aio_read</code>异步读取操作（<code>asynchronous operation</code>）,并将<code>completion handler</code>注册到内核。</li></ul><blockquote><p>在<code>Proactor</code>中我们关心的<code>IO完成事件</code>：内核已经帮我们读好数据并放入我们指定的读缓冲区，用户线程可以直接读取。在<code>Reactor</code>中我们关心的是<code>IO就绪事件</code>：数据已经到来，但是需要用户线程自己去读取。</p></blockquote><ul><li>此时用户线程就可以做其他事情了，无需等待IO结果。而内核与此同时开始异步执行IO操作。当<code>IO操作</code>完成时会产生一个<code>completion event</code>事件，将这个<code>IO完成事件</code>放入<code>completion event queue</code>中。</li><li><code>Proactor</code>从<code>completion event queue</code>中取出<code>completion event</code>，并回调与<code>IO完成事件</code>关联的<code>completion handler</code>。</li><li>在<code>completion handler</code>中完成业务逻辑处理。</li></ul><h3 id="Reactor与Proactor对比"><a href="#Reactor与Proactor对比" class="headerlink" title="Reactor与Proactor对比"></a>Reactor与Proactor对比</h3><ul><li><code>Reactor</code>是基于<code>NIO</code>实现的一种<code>IO线程模型</code>，<code>Proactor</code>是基于<code>AIO</code>实现的<code>IO线程模型</code>。</li><li><code>Reactor</code>关心的是<code>IO就绪事件</code>，<code>Proactor</code>关心的是<code>IO完成事件</code>。</li><li>在<code>Proactor</code>中，用户程序需要向内核传递<code>用户空间的读缓冲区地址</code>。<code>Reactor</code>则不需要。这也就导致了在<code>Proactor</code>中每个并发操作都要求有独立的缓存区，在内存上有一定的开销。</li><li><code>Proactor</code> 的实现逻辑复杂，编码成本较 <code>Reactor</code>要高很多。</li><li><code>Proactor</code> 在处理<code>高耗时 IO</code>时的性能要高于 <code>Reactor</code>，但对于<code>低耗时 IO</code>的执行效率提升<code>并不明显</code>。</li></ul><h2 id="13、Netty的IO模型"><a href="#13、Netty的IO模型" class="headerlink" title="13、Netty的IO模型"></a>13、Netty的IO模型</h2><p>在我们介绍完<code>网络数据包在内核中的收发过程</code>以及五种<code>IO模型</code>和两种<code>IO线程模型</code>后，现在我们来看下<code>netty</code>中的IO模型是什么样的。</p><p>在我们介绍<code>Reactor IO线程模型</code>的时候提到有三种<code>Reactor模型</code>：<code>单Reactor单线程</code>，<code>单Reactor多线程</code>，<code>主从Reactor多线程</code>。</p><p>这三种<code>Reactor模型</code>在<code>netty</code>中都是支持的，但是我们常用的是<code>主从Reactor多线程模型</code>。</p><p>而我们之前介绍的三种<code>Reactor</code>只是一种模型，是一种设计思想。实际上各种网络框架在实现中并不是严格按照模型来实现的，会有一些小的不同，但大体设计思想上是一样的。</p><p>下面我们来看下<code>netty</code>中的<code>主从Reactor多线程模型</code>是什么样子的？</p><p><img src="https://cdn.jsdelivr.net/gh/ZLey373/Pictures@master/640-1727504704762-101.png" alt="图片">netty中的reactor.png</p><ul><li><code>Reactor</code>在<code>netty</code>中是以<code>group</code>的形式出现的，<code>netty</code>中将<code>Reactor</code>分为两组，一组是<code>MainReactorGroup</code>也就是我们在编码中常常看到的<code>EventLoopGroup bossGroup</code>,另一组是<code>SubReactorGroup</code>也就是我们在编码中常常看到的<code>EventLoopGroup workerGroup</code>。</li><li><code>MainReactorGroup</code>中通常只有一个<code>Reactor</code>，专门负责做最重要的事情，也就是监听连接<code>accept</code>事件。当有连接事件产生时，在对应的处理<code>handler acceptor</code>中创建初始化相应的<code>NioSocketChannel</code>（代表一个<code>Socket连接</code>）。然后以<code>负载均衡</code>的方式在<code>SubReactorGroup</code>中选取一个<code>Reactor</code>，注册上去，监听<code>Read事件</code>。</li></ul><blockquote><p><code>MainReactorGroup</code>中只有一个<code>Reactor</code>的原因是，通常我们服务端程序只会<code>绑定监听</code>一个端口，如果要<code>绑定监听</code>多个端口，就会配置多个<code>Reactor</code>。</p></blockquote><ul><li><code>SubReactorGroup</code>中有多个<code>Reactor</code>，具体<code>Reactor</code>的个数可以由系统参数 <code>-D io.netty.eventLoopThreads</code>指定。默认的<code>Reactor</code>的个数为<code>CPU核数 * 2</code>。<code>SubReactorGroup</code>中的<code>Reactor</code>主要负责监听<code>读写事件</code>，每一个<code>Reactor</code>负责监听一组<code>socket连接</code>。将全量的连接<code>分摊</code>在多个<code>Reactor</code>中。</li><li>一个<code>Reactor</code>分配一个<code>IO线程</code>，这个<code>IO线程</code>负责从<code>Reactor</code>中获取<code>IO就绪事件</code>，执行<code>IO调用获取IO数据</code>，执行<code>PipeLine</code>。</li></ul><blockquote><p><code>Socket连接</code>在创建后就被<code>固定的分配</code>给一个<code>Reactor</code>，所以一个<code>Socket连接</code>也只会被一个固定的<code>IO线程</code>执行，每个<code>Socket连接</code>分配一个独立的<code>PipeLine</code>实例，用来编排这个<code>Socket连接</code>上的<code>IO处理逻辑</code>。这种<code>无锁串行化</code>的设计的目的是为了防止多线程并发执行同一个socket连接上的<code>IO逻辑处理</code>，防止出现<code>线程安全问题</code>。同时使系统吞吐量达到最大化</p></blockquote><blockquote><p>由于每个<code>Reactor</code>中只有一个<code>IO线程</code>，这个<code>IO线程</code>既要执行<code>IO活跃Socket连接</code>对应的<code>PipeLine</code>中的<code>ChannelHandler</code>，又要从<code>Reactor</code>中获取<code>IO就绪事件</code>，执行<code>IO调用</code>。所以<code>PipeLine</code>中<code>ChannelHandler</code>中执行的逻辑不能耗时太长，尽量将耗时的业务逻辑处理放入单独的业务线程池中处理，否则会影响其他连接的<code>IO读写</code>，从而近一步影响整个服务程序的<code>IO吞吐</code>。</p></blockquote><ul><li>当<code>IO请求</code>在业务线程中完成相应的业务逻辑处理后，在业务线程中利用持有的<code>ChannelHandlerContext</code>引用将响应数据在<code>PipeLine</code>中反向传播，最终写回给客户端。</li></ul><p><code>netty</code>中的<code>IO模型</code>我们介绍完了，下面我们来简单介绍下在<code>netty</code>中是如何支持前边提到的三种<code>Reactor模型</code>的。</p><h3 id="配置单Reactor单线程"><a href="#配置单Reactor单线程" class="headerlink" title="配置单Reactor单线程"></a>配置单Reactor单线程</h3><pre class="language-none"><code class="language-none">EventLoopGroup eventGroup &#x3D; new NioEventLoopGroup(1);ServerBootstrap serverBootstrap &#x3D; new ServerBootstrap(); serverBootstrap.group(eventGroup);</code></pre><h3 id="配置多Reactor线程"><a href="#配置多Reactor线程" class="headerlink" title="配置多Reactor线程"></a>配置多Reactor线程</h3><pre class="language-none"><code class="language-none">EventLoopGroup eventGroup &#x3D; new NioEventLoopGroup();ServerBootstrap serverBootstrap &#x3D; new ServerBootstrap(); serverBootstrap.group(eventGroup);</code></pre><h3 id="配置主从Reactor多线程"><a href="#配置主从Reactor多线程" class="headerlink" title="配置主从Reactor多线程"></a>配置主从Reactor多线程</h3><pre class="language-none"><code class="language-none">EventLoopGroup bossGroup &#x3D; new NioEventLoopGroup(1); EventLoopGroup workerGroup &#x3D; new NioEventLoopGroup();ServerBootstrap serverBootstrap &#x3D; new ServerBootstrap(); serverBootstrap.group(bossGroup, workerGroup);</code></pre><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文是一篇信息量比较大的文章，用了<code>25</code>张图，<code>22336</code>个字从内核如何处理网络数据包的收发过程开始展开，随后又在<code>内核角度</code>介绍了经常容易混淆的<code>阻塞与非阻塞</code>，<code>同步与异步</code>的概念。以这个作为铺垫，我们通过一个<code>C10K</code>的问题，引出了五种<code>IO模型</code>，随后在<code>IO多路复用</code>中以技术演进的形式介绍了<code>select,poll,epoll</code>的原理和它们综合的对比。最后我们介绍了两种<code>IO线程模型</code>以及<code>netty</code>中的<code>Reactor模型</code>。</p><h2 id="文章来源"><a href="#文章来源" class="headerlink" title="文章来源"></a>文章来源</h2><p>公众号：bin的技术小屋</p><p>原文链接：<a href="https://mp.weixin.qq.com/s/zAh1yD5IfwuoYdrZ1tGf5Q">https://mp.weixin.qq.com/s/zAh1yD5IfwuoYdrZ1tGf5Q</a></p>]]></content>
      
      
      <categories>
          
          <category> 网络编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> netty </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vivo网关实战</title>
      <link href="/2024/09/27/vivo%E7%BD%91%E5%85%B3%E5%AE%9E%E6%88%98/"/>
      <url>/2024/09/27/vivo%E7%BD%91%E5%85%B3%E5%AE%9E%E6%88%98/</url>
      
        <content type="html"><![CDATA[<p>vivo微服务网关的选型和设计：在进行技术选型的时候，主要考虑功能丰富度、性能、稳定性。在反复对比之后，决定选择基于Netty框架进行网关开发；但是考虑到时间的紧迫性，最终选择为针对 Zuul2 进行定制化开发，在 Zuul2 的代码骨架之上去完善网关的整个体系。</p><h2 id="一、背景介绍"><a href="#一、背景介绍" class="headerlink" title="一、背景介绍"></a>一、背景介绍</h2><p>网关作为微服务生态中的重要一环，由于历史原因，中间件团队没有统一的微服务API网关，为此准备技术预研打造一个功能齐全、可用性高的业务网关。</p><h2 id="二、技术选型"><a href="#二、技术选型" class="headerlink" title="二、技术选型"></a>二、技术选型</h2><p>常见的开源网关按照语言分类有如下几类：</p><ul><li>Nginx+Lua：OpenResty、Kong 等；</li><li>Java：Zuul1&#x2F;Zuul2、Spring Cloud Gateway、gravitee-gateway、Dromara Soul 等；</li><li>Go：janus、GoKu API Gateway 等；</li><li>Node.js：Express Gateway、MicroGateway 等。</li></ul><p>由于团队内成员基本上为Java技术栈，因此并不打算深入研究非Java语言的网关。<strong>接下来我们主要调研了Zuul1、Zuul2、Spring Cloud Gateway、Dromara Soul</strong>。</p><p>业界主流的网关基本上可以分为下面三种：</p><ul><li><strong>Servlet + 线程池</strong></li><li><strong>NIO(Tomcat &#x2F; Jetty) + Servlet 3.0 异步</strong></li><li><strong>NettyServer + NettyClient</strong></li></ul><p>在进行技术选型的时候，主要考虑功能丰富度、性能、稳定性。在反复对比之后，决定选择基于Netty框架进行网关开发；但是考虑到时间的紧迫性，最终选择为针对 Zuul2 进行定制化开发，在 Zuul2 的代码骨架之上去完善网关的整个体系。</p><h2 id="三、Zuul2-介绍"><a href="#三、Zuul2-介绍" class="headerlink" title="三、Zuul2 介绍"></a>三、Zuul2 介绍</h2><p>接下来我们简要介绍一下 Zuul2 关键知识点。</p><p>Zuul2 的架构图：</p><p><img src="https://hexoicture.oss-cn-beijing.aliyuncs.com/image/gateway-vivo-1.jpeg" alt="img"></p><p>为了解释上面这张图，接下来会分别介绍几个点</p><ul><li>如何解析 HTTP 协议</li><li>Zuul2 的数据流转</li><li>两个责任链：Netty ChannelPipeline责任链 + Filter责任链</li></ul><h3 id="3-1-如何解析-HTTP-协议"><a href="#3-1-如何解析-HTTP-协议" class="headerlink" title="3.1 如何解析 HTTP 协议"></a>3.1 如何解析 HTTP 协议</h3><blockquote><p>学习Zuul2需要一定的铺垫知识，比如：Google Guice、RxJava、Netflix archaius等，但是更关键的应该是：如何解析HTTP协议，会影响到后续Filter责任链的原理解析，为此先分析这个关键点。</p></blockquote><p>首先我们介绍<a href="https://github.com/Netflix/zuul/wiki/Filters">官方文档在新窗口打开</a>中的一段话：</p><blockquote><p>官方文档</p><p>By default Zuul doesn’t buffer body content, meaning it streams the received headers to the origin before the body has been received.</p><p>默认情况下Zuul2并不会缓存请求体，也就意味着它可能会先发送接收到的请求Headers到后端服务，之后接收到请求体再继续发送到后端服务，发送请求体的时候，也不是组装为一个完整数据之后才发，而是接收到一部分，就转发一部分。</p><p>This streaming behavior is very efficient and desirable, as long as your filter logic depends on header data.</p><p>这个流式行为是高效的，只要Filter过滤的时候只依赖Headers的数据进行逻辑处理，而不需要解析RequestBody。</p></blockquote><p>上面这段话映射到Netty Handler中，则意味着Zuul2并没有使用HttpObjectAggregator。</p><p>我们先看一下常规的Netty Server处理HTTP协议的样例：</p><p>NettyServer样例</p><pre class="language-java" data-language="java"><code class="language-java"><span class="token annotation punctuation">@Slf4j</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">ConfigServerBootstrap</span> <span class="token punctuation">&#123;</span>     <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">final</span> <span class="token keyword">int</span> <span class="token constant">WORKER_THREAD_COUNT</span> <span class="token operator">=</span> <span class="token class-name">Runtime</span><span class="token punctuation">.</span><span class="token function">getRuntime</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">availableProcessors</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">start</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>        <span class="token keyword">int</span> port <span class="token operator">=</span> <span class="token number">8080</span><span class="token punctuation">;</span>        <span class="token class-name">EventLoopGroup</span> bossGroup <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">NioEventLoopGroup</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token class-name">EventLoopGroup</span> workerGroup <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">NioEventLoopGroup</span><span class="token punctuation">(</span><span class="token constant">WORKER_THREAD_COUNT</span><span class="token punctuation">)</span><span class="token punctuation">;</span>         <span class="token keyword">final</span> <span class="token class-name">BizServerHandler</span> bizServerHandler <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">BizServerHandler</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>         <span class="token keyword">try</span> <span class="token punctuation">&#123;</span>            <span class="token class-name">ServerBootstrap</span> serverBootstrap <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">ServerBootstrap</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>             serverBootstrap<span class="token punctuation">.</span><span class="token function">group</span><span class="token punctuation">(</span>bossGroup<span class="token punctuation">,</span> workerGroup<span class="token punctuation">)</span>                    <span class="token punctuation">.</span><span class="token function">channel</span><span class="token punctuation">(</span><span class="token class-name">NioServerSocketChannel</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span>                    <span class="token punctuation">.</span><span class="token function">childHandler</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">ChannelInitializer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Channel</span><span class="token punctuation">></span></span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>                        <span class="token annotation punctuation">@Override</span>                        <span class="token keyword">protected</span> <span class="token keyword">void</span> <span class="token function">initChannel</span><span class="token punctuation">(</span><span class="token class-name">Channel</span> ch<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">Exception</span> <span class="token punctuation">&#123;</span>                            <span class="token class-name">ChannelPipeline</span> pipeline <span class="token operator">=</span> ch<span class="token punctuation">.</span><span class="token function">pipeline</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                            pipeline<span class="token punctuation">.</span><span class="token function">addLast</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">IdleStateHandler</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                            pipeline<span class="token punctuation">.</span><span class="token function">addLast</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">HttpServerCodec</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                            pipeline<span class="token punctuation">.</span><span class="token function">addLast</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">HttpObjectAggregator</span><span class="token punctuation">(</span><span class="token number">500</span> <span class="token operator">*</span> <span class="token number">1024</span> <span class="token operator">*</span> <span class="token number">1024</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                            pipeline<span class="token punctuation">.</span><span class="token function">addLast</span><span class="token punctuation">(</span>bizServerHandler<span class="token punctuation">)</span><span class="token punctuation">;</span>                        <span class="token punctuation">&#125;</span>                    <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            log<span class="token punctuation">.</span><span class="token function">info</span><span class="token punctuation">(</span><span class="token string">"start netty server, port:&#123;&#125;"</span><span class="token punctuation">,</span> port<span class="token punctuation">)</span><span class="token punctuation">;</span>            serverBootstrap<span class="token punctuation">.</span><span class="token function">bind</span><span class="token punctuation">(</span>port<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">sync</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">&#125;</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">InterruptedException</span> e<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>            bossGroup<span class="token punctuation">.</span><span class="token function">shutdownGracefully</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            workerGroup<span class="token punctuation">.</span><span class="token function">shutdownGracefully</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            log<span class="token punctuation">.</span><span class="token function">error</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token string">"start netty server error, port:%s"</span><span class="token punctuation">,</span> port<span class="token punctuation">)</span><span class="token punctuation">,</span> e<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">&#125;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span></code></pre><p>这个例子中的两个关键类为：HttpServerCodec、HttpObjectAggregator。</p><p>HttpServerCodec是HttpRequestDecoder、HttpResponseEncoder的组合器。</p><ul><li><strong>HttpRequestDecoder职责</strong>：将输入的ByteBuf解析成HttpRequest、HttpContent对象。</li><li><strong>HttpResponseEncoder职责</strong>：将HttpResponse、HttpContent对象转换为ByteBuf，进行网络二进制流的输出。</li></ul><p>HttpObjectAggregator的作用：组装HttpMessage、HttpContent为一个完整的FullHttpRequest或者FullHttpResponse。</p><p>当你不想关心chunked分块传输的时候，使用HttpObjectAggregator是非常有用的。</p><p>HTTP协议通常使用Content-Length来标识body的长度，在服务器端，需要先申请对应长度的buffer，然后再赋值。如果需要一边生产数据一边发送数据，就需要使用”Transfer-Encoding: chunked” 来代替Content-Length，也就是对数据进行分块传输。</p><p>接下来我们看一下Zuul2为了解析HTTP协议做了哪些处理。</p><p>Zuul的源码：<a href="https://github.com/Netflix/zuul%EF%BC%8C%E5%9F%BA%E4%BA%8Ev2.1.5%E3%80%82">https://github.com/Netflix/zuul，基于v2.1.5。</a></p><pre class="language-java" data-language="java"><code class="language-java"><span class="token comment">// com.netflix.zuul.netty.server.BaseZuulChannelInitializer#addHttp1Handlers</span><span class="token keyword">protected</span> <span class="token keyword">void</span> <span class="token function">addHttp1Handlers</span><span class="token punctuation">(</span><span class="token class-name">ChannelPipeline</span> pipeline<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    pipeline<span class="token punctuation">.</span><span class="token function">addLast</span><span class="token punctuation">(</span><span class="token constant">HTTP_CODEC_HANDLER_NAME</span><span class="token punctuation">,</span> <span class="token function">createHttpServerCodec</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     pipeline<span class="token punctuation">.</span><span class="token function">addLast</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Http1ConnectionCloseHandler</span><span class="token punctuation">(</span>connCloseDelay<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    pipeline<span class="token punctuation">.</span><span class="token function">addLast</span><span class="token punctuation">(</span><span class="token string">"conn_expiry_handler"</span><span class="token punctuation">,</span>            <span class="token keyword">new</span> <span class="token class-name">Http1ConnectionExpiryHandler</span><span class="token punctuation">(</span>maxRequestsPerConnection<span class="token punctuation">,</span> maxRequestsPerConnectionInBrownout<span class="token punctuation">,</span> connectionExpiry<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token comment">// com.netflix.zuul.netty.server.BaseZuulChannelInitializer#createHttpServerCodec</span><span class="token keyword">protected</span> <span class="token class-name">HttpServerCodec</span> <span class="token function">createHttpServerCodec</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">HttpServerCodec</span><span class="token punctuation">(</span>            <span class="token constant">MAX_INITIAL_LINE_LENGTH</span><span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token constant">MAX_HEADER_SIZE</span><span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token constant">MAX_CHUNK_SIZE</span><span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token boolean">false</span>    <span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span></code></pre><p>通过对比上面的样例发现，Zuul2并没有添加HttpObjectAggregator，也就是需要自行去处理chunked分块传输问题、自行组装请求体数据。</p><p>为了解决上面说的chunked分块传输问题，Zuul2通过判断是否LastHttpContent，来判断是否接收完成。</p><h3 id="3-2-Zuul2-数据流转"><a href="#3-2-Zuul2-数据流转" class="headerlink" title="3.2 Zuul2 数据流转"></a>3.2 Zuul2 数据流转</h3><p><img src="https://hexoicture.oss-cn-beijing.aliyuncs.com/image/gateway-vivo-2.png" alt="img"></p><p>如上图所示，Netty自带的HttpServerCodec会将网络二进制流转换为Netty的HttpRequest对象，再通过ClientRequestReceiver编解码器将HttpRequest转换为Zuul的请求对象HttpRequestMessageImpl；</p><p>请求体RequestBody在Netty自带的HttpServerCodec中被映射为HttpContent对象，ClientRequestReceiver编解码器依次接收HttpContent对象。</p><p>完成了上述数据的转换之后，就流转到了最重要的编解码ZuulFilterChainHandler，里面会执行Filter链，也会发起网络请求到真正的后端服务，这一切都是在ZuulFilterChainHandler中完成的。</p><p>得到了后端服务的响应结果之后，也经过了Outbound Filter的过滤，接下来就是通过ClientResponseWriter把Zuul自定义的响应对象HttpResponseMessageImpl转换为Netty的HttpResponse对象，然后通过HttpServerCodec转换为ByteBuf对象，发送网络二进制流，完成响应结果的输出。</p><p>这里需要特别说明的是：由于Zuul2默认不组装一个完整的请求对象&#x2F;响应对象，所以Zuul2是分别针对请求头+请求Headers、请求体进行Filter过滤拦截的，也就是说对于请求，会走两遍前置Filter链，对于响应结果，也是会走两遍后置Filter链拦截。</p><h3 id="3-3-两个责任链"><a href="#3-3-两个责任链" class="headerlink" title="3.3 两个责任链"></a>3.3 两个责任链</h3><h4 id="3-3-1-Netty-ChannelPipeline责任链"><a href="#3-3-1-Netty-ChannelPipeline责任链" class="headerlink" title="3.3.1 Netty ChannelPipeline责任链"></a>3.3.1 Netty ChannelPipeline责任链</h4><p>Netty的ChannelPipeline设计，通过往ChannelPipeline中动态增减Handler进行定制扩展。</p><p>接下来看一下Zuul2 Netty Server中的pipeline有哪些Handler？</p><p><img src="https://hexoicture.oss-cn-beijing.aliyuncs.com/image/gateway-vivo-3.png" alt="img"></p><p>接着继续看一下Zuul2 Netty Client的Handler有哪些？</p><p><img src="https://hexoicture.oss-cn-beijing.aliyuncs.com/image/gateway-vivo-4.png" alt="img"></p><p>本文不针对具体的Handler进行详细解释，主要是给大家一个整体的视图。</p><h4 id="3-3-2-Filter责任链"><a href="#3-3-2-Filter责任链" class="headerlink" title="3.3.2 Filter责任链"></a>3.3.2 Filter责任链</h4><p><img src="https://hexoicture.oss-cn-beijing.aliyuncs.com/image/gateway-vivo-5.png" alt="img"></p><p>请求发送到Netty Server中，先进行Inbound Filters的拦截处理，接着会调用Endpoint Filter，这里默认为ProxyEndPoint（里面封装了Netty Client），发送请求到真实后端服务，获取到响应结果之后，再执行Outbound Filters，最终返回响应结果。</p><p>三种类型的Filter之间是通过nextStage属性来衔接的。</p><p>Zuul2存在一个定时任务线程GroovyFilterFileManagerPoller，定期扫描特定的目录，通过比对文件的更新时间戳，来判断是否发生变化，如果有变化，则重新编译并放入到内存中。</p><p>通过定位任务实现了Filter的动态加载。</p><h2 id="四、功能介绍"><a href="#四、功能介绍" class="headerlink" title="四、功能介绍"></a>四、功能介绍</h2><p>上面介绍了Zuul2的部分知识点，接下来介绍网关的整体功能。</p><p><img src="https://hexoicture.oss-cn-beijing.aliyuncs.com/image/gateway-vivo-6.png" alt="img"></p><h3 id="4-1-服务注册发现"><a href="#4-1-服务注册发现" class="headerlink" title="4.1 服务注册发现"></a>4.1 服务注册发现</h3><blockquote><p>网关承担了请求转发的功能，需要一定的方法用于动态发现后端服务的机器列表。</p></blockquote><p>这里提供两种方式进行服务的注册发现：</p><p><strong>集成网关SDK</strong></p><ul><li>网关SDK会在服务启动之后，监听ContextRefreshedEvent事件，主动操作zk登记信息到zookeeper注册中心，这样网关服务、网关管理后台就可以订阅节点信息。</li><li>网关SDK添加了ShutdownHook，在服务下线的时候，会删除登记在zk的节点信息，用于通知网关服务、网关管理后台，节点已下线。</li></ul><p><strong>手工配置服务的机器节点信息</strong></p><ul><li>在网关管理后台，手工添加、删除机器节点。</li><li>在网关管理后台，手工设置节点上线、节点下线操。</li></ul><p>为了防止zookeeper故障，网关管理后台已提供HTTP接口用于注册、取消注册作为兜底措施。</p><h3 id="4-2-动态路由"><a href="#4-2-动态路由" class="headerlink" title="4.2 动态路由"></a>4.2 动态路由</h3><blockquote><p>动态路由分为：机房就近路由、灰度路由(类似于Dubbo的标签路由功能)。</p></blockquote><p><strong>机房就近路由</strong>：请求最好是不要跨机房，比如请求打到网关服务的X机房，那么也应该是将请求转发给X机房的后端服务节点，如果后端服务不存在X机房的节点，则请求到其他机房的节点。</p><p><strong>灰度路由</strong>：类似于Dubbo的标签路由功能，如果希望对后端服务节点进行分组隔离，则需要给后端服务一个标签名，建立”标签名→节点列表”的映射关系，请求方携带这个标签名，请求到相应的后端服务节点。</p><p>网关管理后台支持动态配置路由信息，动态开启&#x2F;关闭路由功能。</p><h3 id="4-3-负载均衡"><a href="#4-3-负载均衡" class="headerlink" title="4.3 负载均衡"></a>4.3 负载均衡</h3><p>当前支持的负载均衡策略：加权随机算法、加权轮询算法、一致性哈希算法。</p><p>可以通过网关管理后台动态调整负载均衡策略，支持API接口级别、应用级别的配置。</p><p>负载均衡机制并未采用Netflix Ribbon，而是仿造Dubbo负载均衡的算法实现的。</p><h3 id="4-4-动态配置"><a href="#4-4-动态配置" class="headerlink" title="4.4 动态配置"></a>4.4 动态配置</h3><p>API网关支持一套自洽的动态配置功能，在不依赖第三方配置中心的条件下，仍然支持实时调整配置项，并且配置项分为全局配置、应用级别治理配置、API接口级别治理配置。</p><p>在自洽的动态配置功能之外，网关服务也与公司级别的配置中心进行打通，支持公司级配置中心配置相应的配置项。</p><h3 id="4-5-API管理"><a href="#4-5-API管理" class="headerlink" title="4.5 API管理"></a>4.5 API管理</h3><p>API管理支持网关SDK自动扫描上报，也支持在管理后台手工配置。</p><h3 id="4-6-协议转换"><a href="#4-6-协议转换" class="headerlink" title="4.6 协议转换"></a>4.6 协议转换</h3><p>后端的服务有很多是基于Dubbo框架的，网关服务支持HTTP→HTTP的请求转发，也支持HTTP→Dubbo的协议转换。</p><p>同时C++技术栈，采用了tars框架，网关服务也支持HTTP → tras协议转换。</p><h3 id="4-7-安全机制"><a href="#4-7-安全机制" class="headerlink" title="4.7 安全机制"></a>4.7 安全机制</h3><p>API网关提供了IP黑白名单、OAuth认证授权、appKey&amp;appSecret验签、矛盾加解密、vivo登录态校验的功能。</p><h3 id="4-8-监控-告警"><a href="#4-8-监控-告警" class="headerlink" title="4.8 监控&#x2F;告警"></a>4.8 监控&#x2F;告警</h3><p>API网关通过对接通用监控上报请求访问信息，对API接口的QPS、请求响应吗、请求响应时间等进行监控与告警；</p><p>通过对接基础监控，对网关服务自身节点进行CPU、IO、内存、网络连接等数据进行监控。</p><h3 id="4-9-限流-熔断"><a href="#4-9-限流-熔断" class="headerlink" title="4.9 限流&#x2F;熔断"></a>4.9 限流&#x2F;熔断</h3><p>API网关与限流熔断系统进行打通，可以在限流熔断系统进行API接口级别的配置，比如熔断配置、限流配置，而无需业务系统再次对接限流熔断组件。</p><p>限流熔断系统提供了对Netflix Hystrix、Alibaba Sentinel组件的封装。</p><h3 id="4-10-无损发布"><a href="#4-10-无损发布" class="headerlink" title="4.10 无损发布"></a>4.10 无损发布</h3><p>业务系统的无损发布，这里分为两种场景介绍：</p><ul><li><strong>集成了网关SDK</strong>：网关SDK添加了ShutdownHook，会主动从zookeeper删除登记的节点信息，从而避免请求打到即将下线的节点。</li><li><strong>未集成网关SDK</strong>：如果什么都不做，则只能依赖网关服务的心跳检测功能，会有15s的流量损失。庆幸的是管理后台提供了流量摘除、流量恢复的操作按钮，支持动态的上线、下线机器节点。</li></ul><p>网关集群的无损发布：我们考虑了后端服务的无损发布，但是也需要考虑网关节点自身的无损发布，这里我们不再重复造轮子，直接使用的是CICD系统的HTTP无损发布功能（Nginx动态摘除&#x2F;上线节点）。</p><h3 id="4-11-网关集群分组隔离"><a href="#4-11-网关集群分组隔离" class="headerlink" title="4.11 网关集群分组隔离"></a>4.11 网关集群分组隔离</h3><p>网关集群的分组隔离指的是业务与业务之间的请求应该是隔离的，不应该被部分业务请求打垮了网关服务，从而导致了别的业务请求无法处理。</p><p>这里我们会对接入网关的业务进行分组归类，不同的业务使用不同的分组，不同的网关分组，会部署独立的网关集群，从而隔离了风险，不用再担心业务之间的互相影响。</p><h2 id="五、系统架构"><a href="#五、系统架构" class="headerlink" title="五、系统架构"></a>五、系统架构</h2><h3 id="5-1-模块交互图"><a href="#5-1-模块交互图" class="headerlink" title="5.1 模块交互图"></a>5.1 模块交互图</h3><p><img src="https://hexoicture.oss-cn-beijing.aliyuncs.com/image/gateway-vivo-7.png" alt="img"></p><h3 id="5-2-网关管理后台"><a href="#5-2-网关管理后台" class="headerlink" title="5.2 网关管理后台"></a>5.2 网关管理后台</h3><p>模块划分</p><p><img src="https://hexoicture.oss-cn-beijing.aliyuncs.com/image/gateway-vivo-8.png" alt="img"></p><h3 id="5-3-通信机制"><a href="#5-3-通信机制" class="headerlink" title="5.3 通信机制"></a>5.3 通信机制</h3><blockquote><p>由于需要动态的下发配置，比如全局开关、应用级别的治理配置、接口级别的治理配置，就需要网关管理后台可以与网关服务进行通信，比如推拉模式。</p></blockquote><p><strong>两种设计方案</strong></p><ul><li>基于注册中心的订阅通知机制</li><li>基于HTTP的推模式 + 定时拉取</li></ul><p>这里并未采用第一种方案，主要是因为以下缺点：</p><ul><li>严重依赖zk集群的稳定性</li><li>信息不私密(zk集群权限管控能力较弱、担心被误删)</li><li>无法灰度下发配置，比如只对其中的一台网关服务节点配置生效</li></ul><h4 id="5-3-1-基于HTTP的推模式"><a href="#5-3-1-基于HTTP的推模式" class="headerlink" title="5.3.1 基于HTTP的推模式"></a>5.3.1 基于HTTP的推模式</h4><p><img src="https://hexoicture.oss-cn-beijing.aliyuncs.com/image/gateway-vivo-9.png" alt="img"></p><p>因为Zuul2本身就自带了Netty Server，同理也可以再多启动一个Netty Server提供HTTP服务，让管理后台发送HTTP请求到网关服务，进而发送配置数据到网关服务了。</p><p>所以图上的蓝色标记Netty Server用于接收客户端请求转发到后端节点，紫色标记Netty Server用于提供HTTP服务，接收配置数据。</p><h4 id="5-3-2-全量配置拉取"><a href="#5-3-2-全量配置拉取" class="headerlink" title="5.3.2 全量配置拉取"></a>5.3.2 全量配置拉取</h4><p>网关服务在启动之初，需要发送HTTP请求到管理后台拉取全部的配置数据，并且也需要拉取归属当前节点的灰度配置(只对这个节点生效的试验性配置)。</p><h4 id="5-3-3-增量配置定时拉取"><a href="#5-3-3-增量配置定时拉取" class="headerlink" title="5.3.3 增量配置定时拉取"></a>5.3.3 增量配置定时拉取</h4><blockquote><p>上面提到了”基于HTTP的推模式”进行配置的动态推送，也介绍了全局配置拉取，为了保险起见，网关服务还是新增了一个定时任务，用于定时拉取增量配置。</p></blockquote><p>可以理解为兜底操作，就好比配置中心支持长轮询获取数据实时变更+定时任务获取全部数据。</p><p>在拉取到增量配置之后，会比对内存中的配置数据是否一致，如果一致，则不操作直接丢弃。</p><h4 id="5-3-4-灰度配置下发"><a href="#5-3-4-灰度配置下发" class="headerlink" title="5.3.4 灰度配置下发"></a>5.3.4 灰度配置下发</h4><blockquote><p>上面也提到了”灰度配置”这个词，这里详细解释一下什么是灰度配置？</p></blockquote><p>比如当编辑了某个接口的限流信息，希望在某个网关节点运行一段时间，如果没有问题，则调整配置让全部的网关服务节点生效，如果有问题，则也只是其中一个网关节点的请求流量出问题。</p><p>这样可以降低出错的概率，当某个比较大的改动或者版本上线的时候，可以控制灰度部署一台机器，同时配置也只灰度到这台机器，这样风险就降低了很多。</p><p><strong>灰度配置</strong>：可以理解为只在某些网关节点生效的配置。</p><p>灰度配置下发其实也是通过”5.3.1基于HTTP的推模式”来进行下发的。</p><h3 id="5-4-网关SDK"><a href="#5-4-网关SDK" class="headerlink" title="5.4 网关SDK"></a>5.4 网关SDK</h3><blockquote><p>网关SDK旨在完成后端服务节点的注册与下线、API接口列表数据上报，通过接入网关SDK即可减少手工操作。网关SDK通过 ZooKeeper client操作节点的注册与下线，通过发起HTTP请求进行API接口数据的上报。</p></blockquote><p>支持SpringMVC、SpringBoot的web接口自动扫描、Dubbo新老版本的Service接口扫描。</p><ul><li><strong>Dubbo 接口上报</strong>：</li></ul><ol><li><strong>旧版Dubbo</strong>：自定义BeanPostProcessor，用于提取到ServiceBean，放入线程池异步上报到网关后台。</li><li><strong>新版Dubbo</strong>：自定义ApplicationListener，用于监听ServiceBeanExportedEvent事件，提取event信息，上报到网关后台。</li></ol><ul><li><strong>HTTP 接口上报</strong>：</li></ul><p>自定义BeanPostProcessor，用于提取到Controller、RestController的RequestMapping注解，放入线程池异步上报API信息。</p><h2 id="六、改造之路"><a href="#六、改造之路" class="headerlink" title="六、改造之路"></a>六、改造之路</h2><h3 id="6-1-动态配置"><a href="#6-1-动态配置" class="headerlink" title="6.1 动态配置"></a>6.1 动态配置</h3><p>关联知识点：</p><p><a href="https://github.com/apache/commons-configuration">https://github.com/apache/commons-configuration</a></p><p><a href="https://github.com/Netflix/archaius">https://github.com/Netflix/archaius</a></p><p>Zuul2依赖的动态配置为archaius，通过扩展ConcurrentMapConfiguration添加到ConcurrentCompositeConfiguration中。</p><p>新增GatewayConfigConfiguration，用于存储全局配置、治理配置、节点信息、API数据等。</p><pre class="language-java" data-language="java"><code class="language-java"><span class="token annotation punctuation">@Singleton</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">GatewayConfigConfiguration</span> <span class="token keyword">extends</span> <span class="token class-name">ConcurrentMapConfiguration</span> <span class="token punctuation">&#123;</span>     <span class="token keyword">public</span> <span class="token class-name">GatewayConfigConfiguration</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token comment">/**         * 设置这个值为true，才可以避免archaius强行去除value的类型，导致获取报错         * see com.netflix.config.ConcurrentMapConfiguration#setPropertyImpl(java.lang.String, java.lang.Object)         */</span>        <span class="token keyword">this</span><span class="token punctuation">.</span><span class="token function">setDelimiterParsingDisabled</span><span class="token punctuation">(</span><span class="token class-name">Boolean</span><span class="token punctuation">.</span><span class="token constant">TRUE</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span> <span class="token punctuation">&#125;</span></code></pre><p>通过Google Guice控制Bean的加载顺序，在较早的时机，执行</p><p>ConfigurationManager.getConfigInstance()，获取到ConcurrentCompositeConfiguration，完成GatewayConfigConfiguration的初始化，然后再插入到第一个位置。</p><p>后续只需要对GatewayConfigConfiguration进行配置的增删查改操作即可。</p><h3 id="6-2-路由机制"><a href="#6-2-路由机制" class="headerlink" title="6.2 路由机制"></a>6.2 路由机制</h3><p><img src="https://hexoicture.oss-cn-beijing.aliyuncs.com/image/gateway-vivo-10.png" alt="img"></p><p>路由机制也是仿造的Dubbo路由机制，灰度路由是仿造的Dubbo的标签路由，就近路由可以理解为同机房路由。</p><p><strong>请求处理过程</strong>：</p><p>客户端请求过来的时候，网关服务会通过path前缀提取到对应的后端服务名或者在请求Header中指定传递对应的serviceName，然后只在匹配到的后端服务中，继续API匹配操作，如果匹配到API，则筛选出对应的后端机器列表，然后进行路由、负载均衡，最终选中一台机器，将请求转发过去。</p><p>这里会有个疑问，如果不希望只在某个后端服务中进行请求路由匹配，是希望在一堆后端服务中进行匹配，需要怎么操作？</p><p>在后面的第七章节会解答这个疑问，请耐心阅读。</p><h4 id="6-2-1-就近路由"><a href="#6-2-1-就近路由" class="headerlink" title="6.2.1 就近路由"></a>6.2.1 就近路由</h4><p>当请求到网关服务，会提取网关服务自身的机房loc属性值，读取全局、应用级别的开关，如果就近路由开关打开，则筛选服务列表的时候，会过滤相同loc的后端机器，负载均衡的时候，在相同loc的机器列表中挑选一台进行请求。</p><p>如果没有相同loc的后端机器，则降级从其他loc的后端机器中进行挑选。</p><p><img src="https://hexoicture.oss-cn-beijing.aliyuncs.com/image/gateway-vivo-11.png" alt="img"></p><p>其中 loc信息就是机房信息，每个后端服务节点在SDK上报或者手工录入的时候，都会携带这个值。</p><h4 id="6-2-2-灰度路由"><a href="#6-2-2-灰度路由" class="headerlink" title="6.2.2 灰度路由"></a>6.2.2 灰度路由</h4><p>灰度路由需要用户传递Header属性值，比如<code>gray=canary_gray</code>。</p><p>网关管理后台配置灰度路由的时候，会建立<code>grayName -&gt; List&lt;Server&gt;</code>映射关系，当网关管理后台增量推送到网关服务之后，网关服务就可以通过grayName来提取配置下的后端机器列表，然后再进行负载均衡挑选机器。</p><p>如下图所示：</p><p><img src="https://hexoicture.oss-cn-beijing.aliyuncs.com/image/gateway-vivo-12.png" alt="img"></p><h4 id="6-3-API映射匹配"><a href="#6-3-API映射匹配" class="headerlink" title="6.3 API映射匹配"></a>6.3 API映射匹配</h4><p>网关在进行请求转发的时候，需要明确知道请求哪一个服务的哪一个API，这个过程就是API匹配。</p><p>因为不同的后端服务可能会拥有相同路径的API，所以网关要求请求传递serviceName，serviceName可以放置于请求Header或者请求参数中。</p><p>携带了serviceName之后，就可以在后端服务的API中去匹配了，有一些是相等匹配，有些是正则匹配，因为RESTFul协议，需要支持 <code>/*</code> 通配符匹配。</p><p>这里会有人疑问了，<strong>难道请求一定需要显式传递serviceName吗</strong>？</p><p>为了解决这个问题，创建了一个gateway_origin_mapping表，用于path前缀或者域名前缀 映射到 serviceName，通过在管理后台建立这个映射关系，然后推送到网关服务，即可解决显式传递serviceName的问题，会自动提取请求的path前缀、域名前缀，找到对应的serviceName。</p><p>如果不希望是在一个后端服务中进行API匹配，则需阅读后面的第七章节。</p><h3 id="6-4-负载均衡"><a href="#6-4-负载均衡" class="headerlink" title="6.4 负载均衡"></a>6.4 负载均衡</h3><p>替换 ribbon 组件，改为仿造 Dubbo 的负载均衡机制。</p><pre class="language-java" data-language="java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">interface</span> <span class="token class-name">ILoadBalance</span> <span class="token punctuation">&#123;</span>     <span class="token comment">/**     * 从服务列表中筛选一台机器进行调用     * @param serverList     * @param originName     * @param requestMessage     * @return     */</span>    <span class="token class-name">DynamicServer</span> <span class="token function">select</span><span class="token punctuation">(</span><span class="token class-name">List</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">DynamicServer</span><span class="token punctuation">></span></span> serverList<span class="token punctuation">,</span> <span class="token class-name">String</span> originName<span class="token punctuation">,</span> <span class="token class-name">HttpRequestMessage</span> requestMessage<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">&#125;</span></code></pre><p>替换的理由：ribbon的服务列表更新只是定期更新，如果不考虑复杂的筛选过滤，是满足要求的，但是如果想要灵活的根据请求头、请求参数进行筛选，ribbon则不太适合。</p><h3 id="6-5-心跳检测"><a href="#6-5-心跳检测" class="headerlink" title="6.5 心跳检测"></a>6.5 心跳检测</h3><blockquote><p><strong>核心思路</strong>：当网络请求正常返回的时候，心跳检测是不需要，此时后端服务节点肯定是正常的，只需要定期检测未被请求的后端节点，超过一定的错误阈值，则标记为不可用，从机器列表中剔除。</p></blockquote><p><strong>第一期先实现简单版本</strong>：通过定时任务定期去异步调用心跳检测Url，如果超过失败阈值，则从从负载均衡列表中剔除。</p><p>异步请求采用httpasyncclient组件处理。</p><pre class="language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>org.apache.httpcomponents<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>httpasyncclient<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>4.1.4<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span></code></pre><p>方案为：HealthCheckScheduledExecutor + HealthCheckTask + HttpAsyncClient。</p><h3 id="6-6-日志异步化改造"><a href="#6-6-日志异步化改造" class="headerlink" title="6.6 日志异步化改造"></a>6.6 日志异步化改造</h3><p>Zuul2默认采用的log4j进行日志打印，是同步阻塞操作，需要修改为异步化操作，改为使用logback的AsyncAppender。</p><p>日志打印也是影响性能的一个关键点，需要特别注意，后续会衡量是否切换为log4j2。</p><h3 id="6-7-协议转换"><a href="#6-7-协议转换" class="headerlink" title="6.7 协议转换"></a>6.7 协议转换</h3><ul><li><strong>HTTP -&gt; HTTP</strong></li></ul><p>Zuul2采用的是ProxyEndpoint用于支持HTTP -&gt; HTTP协议转发。</p><p>通过Netty Client的方式发起网络请求到真实的后端服务。</p><ul><li><strong>HTTP -&gt; Dubbo</strong></li></ul><p>采用Dubbo的泛化调用实现HTTP -&gt; Dubbo协议转发，可以采用$invokeAsync。</p><ul><li><strong>HTTP → Tars</strong></li></ul><p>基于tars-java采用类似于Dubbo的泛化调用的方式实现协议转发，基于<a href="https://github.com/TarsCloud/TarsGateway">https://github.com/TarsCloud/TarsGateway</a> 改造而来的。</p><h3 id="6-8-无损发布"><a href="#6-8-无损发布" class="headerlink" title="6.8 无损发布"></a>6.8 无损发布</h3><p>网关作为请求转发，当然希望在业务后端机器部署的期间，不应该把请求转发到还未部署完成的节点。</p><p>业务后端机器节点的无损发布，这里分为两种场景介绍：</p><ul><li><strong>集成了网关SDK</strong></li></ul><p>网关SDK添加了ShutdownHook，会主动从zookeeper删除登记的节点信息，从而避免请求打到即将下线的节点。</p><ul><li><strong>未集成网关SDK</strong></li></ul><p>如果什么都不做，则只能依赖网关服务的心跳检测功能，会有15s的流量损失。庆幸的是管理后台提供了流量摘除、流量恢复的操作按钮，支持动态的上线、下线机器节点。</p><p><strong>设计方案</strong></p><p>我们给后端机器节点dynamic_forward_server表新增了一个字段online，如果online&#x3D;1，则代表在线，接收流量，反之，则代表下线，不接收流量。</p><p>网关服务gateway-server新增一个路由：OnlineRouter，从后端机器列表中筛选online&#x3D;1的机器，过滤掉不在线的机器，则完成了无损发布的功能。</p><pre class="language-java" data-language="java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">interface</span> <span class="token class-name">IRouter</span> <span class="token punctuation">&#123;</span>     <span class="token comment">/**     * 过滤     * @param serverList     * @param originName     * @param requestMessage     * @return     */</span>    <span class="token class-name">List</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">DynamicServer</span><span class="token punctuation">></span></span> <span class="token function">route</span><span class="token punctuation">(</span><span class="token class-name">List</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">DynamicServer</span><span class="token punctuation">></span></span> serverList<span class="token punctuation">,</span> <span class="token class-name">String</span> originName<span class="token punctuation">,</span> <span class="token class-name">HttpRequestMessage</span> requestMessage<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">&#125;</span><span class="token keyword">package</span> <span class="token namespace">com<span class="token punctuation">.</span>netflix<span class="token punctuation">.</span>zuul<span class="token punctuation">.</span>extension<span class="token punctuation">.</span>router</span><span class="token punctuation">;</span> <span class="token keyword">import</span> <span class="token import"><span class="token namespace">com<span class="token punctuation">.</span>netflix<span class="token punctuation">.</span>zuul<span class="token punctuation">.</span>extension<span class="token punctuation">.</span>loadbalance<span class="token punctuation">.</span></span><span class="token class-name">DynamicServer</span></span><span class="token punctuation">;</span><span class="token keyword">import</span> <span class="token import"><span class="token namespace">com<span class="token punctuation">.</span>netflix<span class="token punctuation">.</span>zuul<span class="token punctuation">.</span>message<span class="token punctuation">.</span>http<span class="token punctuation">.</span></span><span class="token class-name">HttpRequestMessage</span></span><span class="token punctuation">;</span><span class="token keyword">import</span> <span class="token import"><span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>commons<span class="token punctuation">.</span>collections<span class="token punctuation">.</span></span><span class="token class-name">CollectionUtils</span></span><span class="token punctuation">;</span> <span class="token keyword">import</span> <span class="token import"><span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">ArrayList</span></span><span class="token punctuation">;</span><span class="token keyword">import</span> <span class="token import"><span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">List</span></span><span class="token punctuation">;</span> <span class="token comment">/** * 在线机器节点_路由 */</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">OnlineRouter</span> <span class="token keyword">implements</span> <span class="token class-name">IRouter</span> <span class="token punctuation">&#123;</span>     <span class="token annotation punctuation">@Override</span>    <span class="token keyword">public</span> <span class="token class-name">List</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">DynamicServer</span><span class="token punctuation">></span></span> <span class="token function">route</span><span class="token punctuation">(</span><span class="token class-name">List</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">DynamicServer</span><span class="token punctuation">></span></span> serverList<span class="token punctuation">,</span> <span class="token class-name">String</span> originName<span class="token punctuation">,</span> <span class="token class-name">HttpRequestMessage</span> requestMessage<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token keyword">boolean</span> hasOfflineServer <span class="token operator">=</span> <span class="token function">hasOfflineMachines</span><span class="token punctuation">(</span>serverList<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>hasOfflineServer<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>            <span class="token comment">// 进行过滤</span>            <span class="token class-name">List</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">DynamicServer</span><span class="token punctuation">></span></span> retServerList <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">ArrayList</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token punctuation">></span></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token class-name">DynamicServer</span> dynamicServer <span class="token operator">:</span> serverList<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>                <span class="token keyword">if</span> <span class="token punctuation">(</span>dynamicServer<span class="token punctuation">.</span><span class="token function">getOnline</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token keyword">null</span> <span class="token operator">||</span> dynamicServer<span class="token punctuation">.</span><span class="token function">getOnline</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">intValue</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>                    retServerList<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span>dynamicServer<span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token punctuation">&#125;</span>            <span class="token punctuation">&#125;</span>            <span class="token keyword">return</span> retServerList<span class="token punctuation">;</span>        <span class="token punctuation">&#125;</span>        <span class="token keyword">return</span> serverList<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>     <span class="token keyword">private</span> <span class="token keyword">boolean</span> <span class="token function">hasOfflineMachines</span><span class="token punctuation">(</span><span class="token class-name">List</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">DynamicServer</span><span class="token punctuation">></span></span> serverList<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token class-name">CollectionUtils</span><span class="token punctuation">.</span><span class="token function">isEmpty</span><span class="token punctuation">(</span>serverList<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>            <span class="token keyword">return</span> <span class="token boolean">false</span><span class="token punctuation">;</span>        <span class="token punctuation">&#125;</span>        <span class="token keyword">boolean</span> hasOfflineServer <span class="token operator">=</span> <span class="token boolean">false</span><span class="token punctuation">;</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token class-name">DynamicServer</span> dynamicServer <span class="token operator">:</span> serverList<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>dynamicServer<span class="token punctuation">.</span><span class="token function">getOnline</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token keyword">null</span> <span class="token operator">&amp;&amp;</span> dynamicServer<span class="token punctuation">.</span><span class="token function">getOnline</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">intValue</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>                hasOfflineServer <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>                <span class="token keyword">break</span><span class="token punctuation">;</span>            <span class="token punctuation">&#125;</span>        <span class="token punctuation">&#125;</span>        <span class="token keyword">return</span> hasOfflineServer<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span> <span class="token punctuation">&#125;</span></code></pre><h3 id="6-9-网关集群分组隔离"><a href="#6-9-网关集群分组隔离" class="headerlink" title="6.9 网关集群分组隔离"></a>6.9 网关集群分组隔离</h3><blockquote><p>网关集群的分组隔离指的是业务与业务之间的请求应该是隔离的，不应该被部分业务请求打垮了网关服务，从而导致了别的业务请求无法处理。</p></blockquote><p>这里我们会对接接入网关的业务进行分组归类，不同的业务使用不同的分组，不同的网关分组，会部署独立的网关集群，从而隔离了风险，不用再担心业务之间的互相影响。</p><p>举例</p><p>金融业务在生产环境存在一个灰度点检环境，为了配合金融业务的迁移，这边也必须有一套独立的环境为之服务，那是否重新部署一套全新的系统呢(独立的前端+独立的管理后台+独立的网关集群)</p><p>其实不必这么操作，我们只需要部署一套独立的网关集群即可，因为网关管理后台，可以同时配置多个网关分组的数据。</p><p>创建一个新的网关分组finance-gray，而新的网关集群只需要拉取finance-gray分组的配置数据即可，不会对其他网关集群造成任何影响。</p><h2 id="七、-如何快速迁移业务"><a href="#七、-如何快速迁移业务" class="headerlink" title="七、.如何快速迁移业务"></a>七、.如何快速迁移业务</h2><p>在业务接入的时候，现有的网关出现了一个尴尬的问题，当某些业务方自行搭建了一套Spring Cloud Gateway网关，里面的服务没有清晰的path前缀、独立的域名拆分，虽然是微服务体系，但是大家共用一个域名，接口前缀也没有良好的划分，混用在一起。</p><p>这个时候如果再按照原有的请求处理流程，则需要业务方进行Nginx的大量修改，需要在location的地方都显式传递serviceName参数，但是业务方不愿意进行这一个调整。</p><p>针对这个问题，其实本质原因在于请求匹配逻辑的不一致性，现有的网关是先匹配服务应用，再进行API匹配，这样效率高一些，而Spring Cloud Gateway则是先API匹配，命中了才知道是哪个后端服务。</p><p>为了解决这个问题，网关再次建立了一个 “微服务集” → “微服务应用列表” 的映射关系，管理后台支持这个映射关系的推送。</p><p>一个网关分组下面会有很多应用服务，这里可以拆分为子集合，可以理解为微服务集就是里面的子集合。</p><p>客户端请求传递过来的时候，需要在请求Header传递 <strong>scTag 参数</strong>，scTag用来标记是哪个微服务集，然后提取到scTag对应的所有后端服务应用列表，依次去对应的应用服务列表中进行API匹配，如果命中了，则代表请求转发到当前应用的后端节点，而对原有的架构改造很小。</p><p>如果不想改动客户端请求，则需要在业务域名的Nginx上进行调整，传递scTag请求Header。</p><h2 id="文章来源"><a href="#文章来源" class="headerlink" title="文章来源"></a>文章来源</h2><p>转载说明:</p><ul><li>作者：Lin Chengjun</li><li>版权声明：本文为「 vivo互联网技术」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。</li><li>原文链接：<a href="https://mp.weixin.qq.com/s/5U1rgpcW21LDYzv8K9EX7g">https://mp.weixin.qq.com/s/5U1rgpcW21LDYzv8K9EX7g</a></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> 系统设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> API网关 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>红黑树</title>
      <link href="/2024/09/24/%E7%BA%A2%E9%BB%91%E6%A0%91-/"/>
      <url>/2024/09/24/%E7%BA%A2%E9%BB%91%E6%A0%91-/</url>
      
        <content type="html"><![CDATA[<p>1、是什么</p><p>红黑树是一种自平衡二叉搜索树，在插入和删除操作后可以通过旋转和重新着色来保持树的平衡</p><p>2、特点</p><ul><li>每个节点都有颜色，红或者黑</li><li>根节点是黑色</li><li>所有的叶子节点是黑色</li><li>每一个红色节点的两个字节点都是黑色</li><li>从根节点出发到叶子节点或空叶子节点的每条路径上，黑色节点的数量是相同的</li></ul><p>3、重点<br>通过以上的五个特性，确保了最长路径不超过最短路径的两倍</p>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
